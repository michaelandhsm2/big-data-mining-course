{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "## Description\n",
    "\n",
    "### Features\n",
    "\n",
    "back,buffer_overflow,ftp_write,guess_passwd,imap,ipsweep,land,loadmodule,multihop,neptune,nmap,normal,perl,phf,pod,portsweep,rootkit,satan,smurf,spy,teardrop,warezclient,warezmaster.\n",
    "\n",
    "0. duration: continuous.\n",
    "0. protocol_type: symbolic.\n",
    "1. service: symbolic.\n",
    "1. flag: symbolic.\n",
    "+ src_bytes: continuous.\n",
    "+ dst_bytes: continuous.\n",
    "+ land: symbolic.\n",
    "+ wrong_fragment: continuous.\n",
    "+ urgent: continuous.\n",
    "+ hot: continuous.\n",
    "+ num_failed_logins: continuous.\n",
    "+ logged_in: symbolic.\n",
    "+ num_compromised: continuous.\n",
    "+ root_shell: continuous.\n",
    "+ su_attempted: continuous.\n",
    "+ num_root: continuous.\n",
    "+ num_file_creations: continuous.\n",
    "+ num_shells: continuous.\n",
    "+ num_access_files: continuous.\n",
    "+ num_outbound_cmds: continuous.\n",
    "+ is_host_login: symbolic.\n",
    "+ is_guest_login: symbolic.\n",
    "+ count: continuous.\n",
    "+ srv_count: continuous.\n",
    "+ serror_rate: continuous.\n",
    "+ srv_serror_rate: continuous.\n",
    "+ rerror_rate: continuous.\n",
    "+ srv_rerror_rate: continuous.\n",
    "+ same_srv_rate: continuous.\n",
    "+ diff_srv_rate: continuous.\n",
    "+ srv_diff_host_rate: continuous.\n",
    "+ dst_host_count: continuous.\n",
    "+ dst_host_srv_count: continuous.\n",
    "+ dst_host_same_srv_rate: continuous.\n",
    "+ dst_host_diff_srv_rate: continuous.\n",
    "+ dst_host_same_src_port_rate: continuous.\n",
    "+ dst_host_srv_diff_host_rate: continuous.\n",
    "+ dst_host_serror_rate: continuous.\n",
    "+ dst_host_srv_serror_rate: continuous.\n",
    "+ dst_host_rerror_rate: continuous.\n",
    "+ dst_host_srv_rerror_rate: continuous.\n",
    "+ intrusion_type: symbolic\n",
    "\n",
    "### Task\n",
    "\n",
    "\n",
    "#### Primary tasks\n",
    "\n",
    "* (32pt) (1) For continuous attributes ‘duration’, ‘src_bytes’, ‘dst_bytes’,‘num_failed_logins’, please calculate their mean, median, mode, standard deviation, respectively\n",
    "\n",
    "\n",
    "* (20pt) (2) For symbolic attributes ‘protocol_type’, ‘service’, ‘flag’, ‘logged_in’, ‘intrusion_type’, output the list of each value and the corresponding frequency count, sorted in descending order of the count\n",
    "\n",
    "\n",
    "* (20pt) (3) Output the list of the most frequently used ‘service’ for each ‘intrusion_type’, sorted in descending order of the occurrence frequency\n",
    "\n",
    "\n",
    "* (30pt) (4) If we regard the values of ‘intrusion_type’ except “normal” as abnormal, calculate the correlation coefficient of “number of abnormal instances” and ‘num_failed_logins’ by the following formula: \n",
    "\n",
    "#### Advanced task: (optional bonus)\n",
    "* (35pt) (5) Which ‘intrusion type’ has the highest value for each of the following fields:\n",
    "    * same_srv_rate\n",
    "    * diff_srv_rate\n",
    "    * srv_diff_host_rate\n",
    "    + dst_host_count\n",
    "    + dst_host_srv_count\n",
    "    + dst_host_same_srv_rate\n",
    "    + dst_host_diff_srv_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@5ef44a3d\n",
      "Spark version:      2.2.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "getFile: (fileString: String)Array[String]\n",
       "outputWriter: (fileString: String)java.io.PrintWriter\n",
       "printSample: (writer: Any, data: Any, title: String, format: String)Unit\n",
       "checkArgs: (args: Array[String], requiredArgs: Int, style: String)Unit\n",
       "printSpark: (writer: Any, spark: org.apache.spark.sql.SparkSession)Unit\n",
       "writer: Null = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.hadoop.fs._\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import java.io.{File,PrintWriter}\n",
    "\n",
    "  def printSample(writer: Any, data: Any, title: String = \"\", format: String = \"\"){\n",
    "    println(\"\\n\"+title+\" Data Sample: \" + format)\n",
    "    println(data+\"\\n\")\n",
    "  }\n",
    "\n",
    "  def printSpark(writer: Any, spark: SparkSession): Unit = {\n",
    "    println(\"Spark Entity:       \" + spark)\n",
    "    println(\"Spark version:      \" + spark.version)\n",
    "    println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "    println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)\n",
    "    println(\"\")\n",
    "  }\n",
    "\n",
    "val writer = null\n",
    "printSpark(writer, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@5ef44a3d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://10.128.0.2:4041)\" target=\"new_tab\">Spark UI: local-1524287638063</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1524287638063: Some(http://10.128.0.2:4041)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark2 = spark\n",
    "import spark2.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 15:===================================================>    (21 + 1) / 23]\n",
      "Continuous Sample Data Sample: \n",
      "0,tcp,http,SF,217,2445,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,4,4,0.00,0.00,0.00,0.00,1.00,0.00,0.00,4,255,1.00,0.00,0.25,0.01,0.00,0.00,0.00,0.00,normal.\n",
      "0,icmp,ecr_i,SF,1032,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,511,511,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,255,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,smurf.\n",
      "0,tcp,private,S0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,277,17,1.00,1.00,0.00,0.00,0.06,0.06,0.00,255,17,0.07,0.07,0.00,0.00,1.00,1.00,0.00,0.00,neptune.\n",
      "0,icmp,ecr_i,SF,520,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,448,448,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,255,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,smurf.\n",
      "0,tcp,private,REJ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,206,3,0.00,0.00,1.00,1.00,0.01,0.07,0.00,255,3,0.01,0.08,0.00,0.00,0.00,0.00,1.00,1.00,neptune.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = ./data/kddcup.data MapPartitionsRDD[18] at textFile at <console>:59\n",
       "sampleData = ParallelCollectionRDD[20] at parallelize at <console>:61\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[20] at parallelize at <console>:61"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.sparkContext.textFile(\"./data/kddcup.data\")\n",
    "\n",
    "val sampleData = spark.sparkContext.parallelize(data.takeSample(false, 30, System.nanoTime.toInt))\n",
    "printSample(writer, sampleData.take(5).mkString(\"\\n\"), \"Continuous Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Task 1 \n",
    "For continuous attributes ‘duration’(0), ‘src_bytes’(4), ‘dst_bytes’(5),‘num_failed_logins’(10), please calculate their mean, median, mode, standard deviation, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuous Sample Data Sample: \n",
      "(duration,0.0)\n",
      "(src_bytes,217.0)\n",
      "(dst_bytes,2445.0)\n",
      "(num_failed_logins,0.0)\n",
      "(duration,0.0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "continous = Array(duration, src_bytes, dst_bytes, num_failed_logins)\n",
       "flattenContinuousData = MapPartitionsRDD[21] at flatMap at <console>:69\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[21] at flatMap at <console>:69"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//TODO: SampleData => Data\n",
    "val continous = Array(\"duration\", \"src_bytes\", \"dst_bytes\", \"num_failed_logins\")\n",
    "\n",
    "var flattenContinuousData = sampleData.\n",
    "    flatMap{ dataString =>\n",
    "        dataString.split(\",\").\n",
    "            zipWithIndex.\n",
    "            map{\n",
    "                case (value,index) => \n",
    "                    index match{\n",
    "                        case 0 => (continous(0),value.toString.toDouble)\n",
    "                        case 4 => (continous(1),value.toString.toDouble)\n",
    "                        case 5 => (continous(2),value.toString.toDouble)\n",
    "                        case 10 => (continous(3),value.toString.toDouble)\n",
    "                        case _ => (\"\",0.0)\n",
    "                    }\n",
    "                }.\n",
    "            filter(x => x!=(\"\",0.0))\n",
    "    }\n",
    "printSample(writer, flattenContinuousData.take(5).mkString(\"\\n\"), \"Continuous Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count Sample Data Sample: \n",
      "Map(src_bytes -> 30, dst_bytes -> 30, num_failed_logins -> 30, duration -> 30)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countMap = Map(src_bytes -> 30, dst_bytes -> 30, num_failed_logins -> 30, duration -> 30)\n",
       "countBroadcast = Broadcast(26)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(26)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var countMap = flattenContinuousData.map{case (k,v) => (k,1)}.reduceByKey((i, j) => i+j).collectAsMap()\n",
    "var countBroadcast = spark.sparkContext.broadcast(countMap)\n",
    "printSample(writer, countBroadcast.value, \"Count Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Sample Data Sample: \n",
      "Map(src_bytes -> 454.76666666666665, dst_bytes -> 712.9666666666667, num_failed_logins -> 0.0, duration -> 394.6666666666667)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "averageMap = Map(src_bytes -> 454.76666666666665, dst_bytes -> 712.9666666666667, num_failed_logins -> 0.0, duration -> 394.6666666666667)\n",
       "averageBroadcast = Broadcast(29)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(29)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var averageMap = flattenContinuousData.reduceByKey((i, j) => i+j).\n",
    "                                       map{case (k, v) => (k,v/countBroadcast.value(k))}.collectAsMap()\n",
    "var averageBroadcast = spark.sparkContext.broadcast(averageMap)\n",
    "printSample(writer, averageBroadcast.value, \"Average Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard Deviation Sample Data Sample: \n",
      "Map(src_bytes -> 438.3953834408184, dst_bytes -> 2706.7507887173915, num_failed_logins -> 0.0, duration -> 1725.2439119021851)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stdMap = Map(src_bytes -> 438.3953834408184, dst_bytes -> 2706.7507887173915, num_failed_logins -> 0.0, duration -> 1725.2439119021851)\n",
       "stdBroadcast = Broadcast(32)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stdMap = flattenContinuousData.\n",
    "    map{case (k,v) => (k, scala.math.pow(v-averageBroadcast.value(k),2))}.\n",
    "    reduceByKey((i,j) => i+j).\n",
    "    map{case (k,v) => (k, math.sqrt(v/countBroadcast.value(k)))}.collectAsMap()\n",
    "var stdBroadcast = spark.sparkContext.broadcast(stdMap)\n",
    "printSample(writer, stdBroadcast.value, \"Standard Deviation Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode Sample Data Sample: \n",
      "Map(src_bytes -> Vector(1032.0), dst_bytes -> Vector(0.0), num_failed_logins -> Vector(0.0), duration -> Vector(0.0))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modeMap = Map(src_bytes -> Vector(1032.0), dst_bytes -> Vector(0.0), num_failed_logins -> Vector(0.0), duration -> Vector(0.0))\n",
       "modeBroadcast = Broadcast(44)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(44)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var modeMap = flattenContinuousData.\n",
    "                map{case(k, v) => ((k,v),1)}.\n",
    "                reduceByKey((i, j) => i+j).\n",
    "                map{case((k,v),count) => (k,(v,count))}.\n",
    "                groupByKey.mapValues{ iterator =>\n",
    "                    iterator.toVector.sortBy { case(v, count) => \n",
    "                        (-count, v)\n",
    "                    }.take(1).map{\n",
    "                        case(v,count) => v \n",
    "                    }\n",
    "                }.collectAsMap()\n",
    "var modeBroadcast = spark.sparkContext.broadcast(modeMap)\n",
    "printSample(writer, modeBroadcast.value, \"Mode Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median Sample Data Sample: \n",
      "Map(src_bytes -> 220.0, dst_bytes -> 0.0, num_failed_logins -> 0.0, duration -> 0.0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "medianMap = Map(src_bytes -> 220.0, dst_bytes -> 0.0, num_failed_logins -> 0.0, duration -> 0.0)\n",
       "medianBroadcast = Broadcast(84)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(84)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var medianMap = flattenContinuousData.\n",
    "                groupByKey.map{ case (k,iterator) =>\n",
    "                    val medianVector = iterator.toVector.sortBy { case(count) => \n",
    "                        (-count)\n",
    "                    }\n",
    "                    val num = (countBroadcast.value(k)/2).toInt\n",
    "                    (k,medianVector(num))\n",
    "                }.collectAsMap()\n",
    "var medianBroadcast = spark.sparkContext.broadcast(medianMap)\n",
    "printSample(writer, medianBroadcast.value, \"Median Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For duration:\n",
      "        Number of Meaningful Data - 30\n",
      "        Mean - 394.6666666666667\n",
      "        Standard Deviation - 1725.2439119021851\n",
      "        Mode - Vector(0.0)\n",
      "        Median - 0.0\n",
      "\n",
      "For src_bytes:\n",
      "        Number of Meaningful Data - 30\n",
      "        Mean - 454.76666666666665\n",
      "        Standard Deviation - 438.3953834408184\n",
      "        Mode - Vector(1032.0)\n",
      "        Median - 220.0\n",
      "\n",
      "For dst_bytes:\n",
      "        Number of Meaningful Data - 30\n",
      "        Mean - 712.9666666666667\n",
      "        Standard Deviation - 2706.7507887173915\n",
      "        Mode - Vector(0.0)\n",
      "        Median - 0.0\n",
      "\n",
      "For num_failed_logins:\n",
      "        Number of Meaningful Data - 30\n",
      "        Mean - 0.0\n",
      "        Standard Deviation - 0.0\n",
      "        Mode - Vector(0.0)\n",
      "        Median - 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.ArrayIndexOutOfBoundsException\n",
       "Message: 4\n",
       "StackTrace:   at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def task1Print(s: String): Unit = {\n",
    "    println(\"For \"+s+\":\")\n",
    "    println(\"        Number of Meaningful Data - \" + countBroadcast.value(s))\n",
    "    println(\"        Mean - \" + averageBroadcast.value(s))\n",
    "    println(\"        Standard Deviation - \" + stdBroadcast.value(s))\n",
    "    println(\"        Mode - \" + modeBroadcast.value(s))\n",
    "    println(\"        Median - \" + medianBroadcast.value(s)+ \"\\n\")\n",
    "} \n",
    "\n",
    "var i = 0;\n",
    "for(i <- 0 to continous.length){\n",
    "    task1Print(continous(i))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "For symbolic attributes ‘protocol_type’(1), ‘service’(2), ‘flag’(3), ‘logged_in’(11), ‘intrusion_type’(41), output the list of each value and the corresponding frequency count, sorted in descending order of the count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 153, localhost, executor driver): java.lang.NumberFormatException: For input string: \"tcp\"\n",
       "\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
       "\tat sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
       "\tat java.lang.Double.parseDouble(Double.java:538)\n",
       "\tat scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284)\n",
       "\tat scala.collection.immutable.StringOps.toDouble(StringOps.scala:29)\n",
       "\tat $line223.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1$$anonfun$apply$1.apply(<console>:71)\n",
       "\tat $line223.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1$$anonfun$apply$1.apply(<console>:68)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
       "\tat $line223.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:68)\n",
       "\tat $line223.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:65)\n",
       "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
       "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat $anonfun$1$$anonfun$apply$1.apply(<console>:68)\n",
       "\tat $anonfun$1.apply(<console>:68)\n",
       "\tat $anonfun$1.apply(<console>:65)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.take(RDD.scala:1327)\n",
       "  ... 50 elided\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"tcp\"\n",
       "  at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n",
       "  at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n",
       "  at java.lang.Double.parseDouble(Double.java:538)\n",
       "  at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284)\n",
       "  at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29)\n",
       "  at $anonfun$1$$anonfun$apply$1.apply(<console>:71)\n",
       "  at $anonfun$1$$anonfun$apply$1.apply(<console>:68)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
       "  at $anonfun$1.apply(<console>:68)\n",
       "  at $anonfun$1.apply(<console>:65)\n",
       "  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n",
       "  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
       "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "  at scala.collection.AbstractIterator.to(Iterator.scala:1336)\n",
       "  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n",
       "  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//TODO: SampleData => Data\n",
    "val discrete = Array(\"protocol_type\", \"service\", \"flag\", \"logged_in\", \"intrusion_type\")\n",
    "\n",
    "var flattenDiscreteData = sampleData.\n",
    "    flatMap{ dataString =>\n",
    "        dataString.split(\",\").\n",
    "            zipWithIndex.\n",
    "            map{\n",
    "                case (value,index) => \n",
    "                    index match{\n",
    "                        case 1 => (discrete(0),value.toString)\n",
    "                        case 2 => (discrete(1),value.toString)\n",
    "                        case 3 => (discrete(2),value.toString)\n",
    "                        case 11 => (discrete(3),value.toString)\n",
    "                        case 41 => (discrete(4),value.toString.replace)\n",
    "                        case _ => (\"\",0.0)\n",
    "                    }\n",
    "                }.\n",
    "            filter(x => x!=(\"\",0.0))\n",
    "    }\n",
    "printSample(writer, flattenDiscreteData.take(5).mkString(\"\\n\"), \"Discrete Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var modeMap = flattenContinuousData.\n",
    "                map{case(k, v) => ((k,v),1)}.\n",
    "                reduceByKey((i, j) => i+j).\n",
    "                map{case((k,v),count) => (k,(v,count))}.\n",
    "                groupByKey.mapValues{ iterator =>\n",
    "                    iterator.toVector.sortBy { case(v, count) => \n",
    "                        (-count, v)\n",
    "                    }.take(1).map{\n",
    "                        case(v,count) => v \n",
    "                    }\n",
    "                }.collectAsMap()\n",
    "var modeBroadcast = spark.sparkContext.broadcast(modeMap)\n",
    "printSample(writer, modeBroadcast.value, \"Mode Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val count = flattenData.map{case (k,v) => (k,1)}.reduceByKey((i, j) => i+j).collectAsMap()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val max = flattenData.reduceByKey{(i, j) => if (i>j) i else j}.collectAsMap()\n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val min = flattenData.reduceByKey{(i, j) => if (i<j) i else j}.collectAsMap()\n",
    "min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Mean & Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val average = flattenData.reduceByKey((i, j) => i+j).map{case (i, j) => (i,j/2049280)}.collectAsMap()\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val std = flattenData.\n",
    "    map{case (k,v) => (k, scala.math.pow(v-average(k),2))}.\n",
    "    reduceByKey((i,j) => i+j).\n",
    "    map{case (k,v) => (k, math.sqrt(v/count(k)))}.collectAsMap()\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Min-Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val norm = flattenData.take(10).map{case (k,v) => (k, (v-min(k))/(max(k)-min(k)))}\n",
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object MyFunctions {\n",
    "    def myprint(s: String): Unit = {\n",
    "        println(\"For \"+s+\":\")\n",
    "        println(\"        Number of Meaningful Data - \" + count(s))\n",
    "        println(\"        Maximum Value - \" + max(s))\n",
    "        println(\"        Minimum Value - \" + min(s))\n",
    "        println(\"        Mean - \" + average(s))\n",
    "        println(\"        Standard Deviation - \" + std(s) + \"\\n\")\n",
    "    }    \n",
    "}\n",
    "\n",
    "MyFunctions.myprint(\"Global Active Power\")\n",
    "MyFunctions.myprint(\"Global Reactive Power\")\n",
    "MyFunctions.myprint(\"Voltage\")\n",
    "MyFunctions.myprint(\"Global Intensity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "val spark2 = spark\n",
    "import spark2.implicits._\n",
    "\n",
    "object Norm {\n",
    "    def AP(s: String): Double = {\n",
    "        process(s, \"Global Active Power\")\n",
    "    } \n",
    "    \n",
    "    def RP(s: String): Double = {\n",
    "        process(s, \"Global Reactive Power\")\n",
    "    } \n",
    "    \n",
    "    def V(s: String): Double = {\n",
    "        process(s, \"Voltage\")\n",
    "    } \n",
    "    \n",
    "    def I(s: String): Double = {\n",
    "        process(s, \"Global Intensity\")\n",
    "    } \n",
    "    \n",
    "    def process(s: String, k: String): Double = {\n",
    "        var retVal = 0.0\n",
    "        if(s == \"?\"){\n",
    "            retVal = Double.NaN\n",
    "        }else{\n",
    "            retVal = (s.toDouble-min(k))/(max(k)-min(k))\n",
    "        }\n",
    "        retVal\n",
    "    }\n",
    "}\n",
    "\n",
    "val dataDF = rows.\n",
    "                map(_.split(\";\")).\n",
    "                map(att => (att(0), att(1), Norm.AP(att(2)), Norm.RP(att(3)), Norm.V(att(4)), Norm.I(att(5)) )).\n",
    "                toDF(\"Date\", \"Time\", \"Active_Power\", \"Reactive_Power\",\"Voltage\",\"Intensity\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.write.csv(\"./data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.createOrReplaceTempView(\"records\")\n",
    "spark.sql(\"SELECT * FROM records WHERE date = '28/4/2007'\").show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
