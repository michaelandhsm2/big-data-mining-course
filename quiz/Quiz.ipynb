{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "## Description\n",
    "\n",
    "### Features\n",
    "\n",
    "back,buffer_overflow,ftp_write,guess_passwd,imap,ipsweep,land,loadmodule,multihop,neptune,nmap,normal,perl,phf,pod,portsweep,rootkit,satan,smurf,spy,teardrop,warezclient,warezmaster.\n",
    "\n",
    "0. duration: continuous.\n",
    "0. protocol_type: symbolic.\n",
    "1. service: symbolic.\n",
    "1. flag: symbolic.\n",
    "+ src_bytes: continuous.\n",
    "+ dst_bytes: continuous.\n",
    "+ land: symbolic.\n",
    "+ wrong_fragment: continuous.\n",
    "+ urgent: continuous.\n",
    "+ hot: continuous.\n",
    "+ num_failed_logins: continuous.\n",
    "+ logged_in: symbolic.\n",
    "+ num_compromised: continuous.\n",
    "+ root_shell: continuous.\n",
    "+ su_attempted: continuous.\n",
    "+ num_root: continuous.\n",
    "+ num_file_creations: continuous.\n",
    "+ num_shells: continuous.\n",
    "+ num_access_files: continuous.\n",
    "+ num_outbound_cmds: continuous.\n",
    "+ is_host_login: symbolic.\n",
    "+ is_guest_login: symbolic.\n",
    "+ count: continuous.\n",
    "+ srv_count: continuous.\n",
    "+ serror_rate: continuous.\n",
    "+ srv_serror_rate: continuous.\n",
    "+ rerror_rate: continuous.\n",
    "+ srv_rerror_rate: continuous.\n",
    "+ same_srv_rate: continuous.\n",
    "+ diff_srv_rate: continuous.\n",
    "+ srv_diff_host_rate: continuous.\n",
    "+ dst_host_count: continuous.\n",
    "+ dst_host_srv_count: continuous.\n",
    "+ dst_host_same_srv_rate: continuous.\n",
    "+ dst_host_diff_srv_rate: continuous.\n",
    "+ dst_host_same_src_port_rate: continuous.\n",
    "+ dst_host_srv_diff_host_rate: continuous.\n",
    "+ dst_host_serror_rate: continuous.\n",
    "+ dst_host_srv_serror_rate: continuous.\n",
    "+ dst_host_rerror_rate: continuous.\n",
    "+ dst_host_srv_rerror_rate: continuous.\n",
    "\n",
    "### Task\n",
    "\n",
    "\n",
    "#### Primary tasks\n",
    "\n",
    "* (32pt) (1) For continuous attributes ‘duration’, ‘src_bytes’, ‘dst_bytes’,‘num_failed_logins’, please calculate their mean, median, mode, standard deviation, respectively\n",
    "\n",
    "\n",
    "* (20pt) (2) For symbolic attributes ‘protocol_type’, ‘service’, ‘flag’, ‘logged_in’, ‘intrusion_type’, output the list of each value and the corresponding frequency count, sorted in descending order of the count\n",
    "\n",
    "\n",
    "* (20pt) (3) Output the list of the most frequently used ‘service’ for each ‘intrusion_type’, sorted in descending order of the occurrence frequency\n",
    "\n",
    "\n",
    "* (30pt) (4) If we regard the values of ‘intrusion_type’ except “normal” as abnormal, calculate the correlation coefficient of “number of abnormal instances” and ‘num_failed_logins’ by the following formula: \n",
    "\n",
    "#### Advanced task: (optional bonus)\n",
    "* (35pt) (5) Which ‘intrusion type’ has the highest value for each of the following fields:\n",
    "    * same_srv_rate\n",
    "    * diff_srv_rate\n",
    "    * srv_diff_host_rate\n",
    "    + dst_host_count\n",
    "    + dst_host_srv_count\n",
    "    + dst_host_same_srv_rate\n",
    "    + dst_host_diff_srv_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@4487574e\n",
      "Spark version:      2.2.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object Common\n",
       "writer = java.io.PrintWriter@1ead7c5e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "java.io.PrintWriter@1ead7c5e"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.hadoop.fs._\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import java.io.{File,PrintWriter}\n",
    "\n",
    "object Common{\n",
    "  def getFile(fileString: String): Array[String] ={\n",
    "    val inputPath = new Path(fileString)\n",
    "    val inputBuffer = scala.collection.mutable.ArrayBuffer.empty[String]\n",
    "    val iterator = inputPath.getFileSystem(new Configuration()).listFiles(inputPath, false)\n",
    "    while(iterator.hasNext()){\n",
    "        val fileStatus = iterator.next()\n",
    "        if(fileStatus.isFile()){\n",
    "          inputBuffer += fileStatus.getPath().toString()\n",
    "        }\n",
    "    }\n",
    "    inputBuffer.toArray\n",
    "  }\n",
    "\n",
    "  def outputWriter(fileString: String): PrintWriter ={\n",
    "    val outputPath = new Path(fileString)\n",
    "    val outputStream = outputPath.getFileSystem(new Configuration()).create(outputPath);\n",
    "    new PrintWriter(outputStream)\n",
    "  }\n",
    "\n",
    "  def printSample(writer: PrintWriter, data: Any, title: String = \"\", format: String = \"\"){\n",
    "    println(title+\" Data Sample: \" + format)\n",
    "    println(data+\"\\n\")\n",
    "  }\n",
    "\n",
    "  def checkArgs(args: Array[String], requiredArgs: Int, style: String): Unit = {\n",
    "    if (args.length < requiredArgs) {\n",
    "      System.err.println(s\"\\nThis program expects $requiredArgs arguments.\")\n",
    "      System.err.println(s\"Usage: -- $style\\n\")\n",
    "      System.exit(1)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def printSpark(writer: PrintWriter, spark: SparkSession): Unit = {\n",
    "    println(\"Spark Entity:       \" + spark)\n",
    "    println(\"Spark version:      \" + spark.version)\n",
    "    println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "    println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)\n",
    "    println(\"\")\n",
    "  }\n",
    "}\n",
    "\n",
    "val writer = Common.outputWriter(\"./output.txt\")\n",
    "Common.printSpark(writer, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@4487574e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://10.128.0.2:4040)\" target=\"new_tab\">Spark UI: local-1524283277846</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1524283277846: Some(http://10.128.0.2:4040)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark2 = spark\n",
    "import spark2.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================================>    (21 + 1) / 23]Sample Data Sample: \n",
      "0,tcp,private,S0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,260,3,1.00,1.00,0.00,0.00,0.01,0.06,0.00,255,3,0.01,0.07,0.00,0.00,1.00,1.00,0.00,0.00,neptune.\n",
      "0,icmp,ecr_i,SF,1032,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,511,511,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,255,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,smurf.\n",
      "0,tcp,http,SF,309,720,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,14,14,0.00,0.00,0.00,0.00,1.00,0.00,0.00,14,255,1.00,0.00,0.07,0.04,0.00,0.00,0.00,0.00,normal.\n",
      "0,icmp,ecr_i,SF,1032,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,510,510,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,255,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,smurf.\n",
      "0,icmp,ecr_i,SF,1032,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,509,509,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,255,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,smurf.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = ./data/kddcup.data MapPartitionsRDD[1] at textFile at <console>:43\n",
       "sampleData = ParallelCollectionRDD[3] at parallelize at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[3] at parallelize at <console>:45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.sparkContext.textFile(\"./data/kddcup.data\")\n",
    "\n",
    "val sampleData = spark.sparkContext.parallelize(data.takeSample(false, 30, System.nanoTime.toInt))\n",
    "Common.printSample(writer, sampleData.take(5).mkString(\"\\n\"), \"Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Task 1 \n",
    "For continuous attributes ‘duration’(0), ‘src_bytes’(4), ‘dst_bytes’(5),‘num_failed_logins’(10), please calculate their mean, median, mode, standard deviation, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Sample Data Sample: \n",
      "(duration,0.0)\n",
      "(src_bytes,0.0)\n",
      "(dst_bytes,0.0)\n",
      "(num_failed_logins,0.0)\n",
      "(duration,0.0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "continous = Array(duration, src_bytes, dst_bytes, num_failed_logins)\n",
       "flattenContinuousData = MapPartitionsRDD[4] at flatMap at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[4] at flatMap at <console>:50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//TODO: SampleData => Data\n",
    "val continous = Array(\"duration\", \"src_bytes\", \"dst_bytes\", \"num_failed_logins\")\n",
    "\n",
    "var flattenContinuousData = sampleData.\n",
    "    flatMap{ dataString =>\n",
    "        dataString.split(\",\").\n",
    "            zipWithIndex.\n",
    "            map{\n",
    "                case (value,index) => \n",
    "                    index match{\n",
    "                        case 0 => (continous(0),value.toString.toDouble)\n",
    "                        case 4 => (continous(1),value.toString.toDouble)\n",
    "                        case 5 => (continous(2),value.toString.toDouble)\n",
    "                        case 10 => (continous(3),value.toString.toDouble)\n",
    "                        case _ => (\"\",0.0)\n",
    "                    }\n",
    "                }.\n",
    "            filter(x => x!=(\"\",0.0)).toSeq\n",
    "    }\n",
    "Common.printSample(writer, flattenContinuousData.take(5).mkString(\"\\n\"), \"Continuous Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Sample Data Sample: \n",
      "Map(src_bytes -> 30, dst_bytes -> 30, num_failed_logins -> 30, duration -> 30)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countMap = Map(src_bytes -> 30, dst_bytes -> 30, num_failed_logins -> 30, duration -> 30)\n",
       "count = Broadcast(7)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var countMap = flattenContinuousData.map{case (k,v) => (k,1)}.reduceByKey((i, j) => i+j).collectAsMap()\n",
    "var count = spark.sparkContext.broadcast(\"hi\")\n",
    "Common.printSample(writer, countMap, \"Count Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hi"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2287)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.map(RDD.scala:369)\n",
       "  ... 46 elided\n",
       "Caused by: java.io.NotSerializableException: Common$\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: Common$, value: Common$@146b6344)\n",
       "\t- field (class: $iw, name: Common$module, type: class Common$)\n",
       "\t- object (class $iw, $iw@75d4b24f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4da1638)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1855ae81)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5fcc5d47)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@623b8339)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@164480e1)\n",
       "\t- field (class: $line13.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line13.$read, $line13.$read@30056fee)\n",
       "\t- field (class: $iw, name: $line13$read, type: class $line13.$read)\n",
       "\t- object (class $iw, $iw@5cc6c0e)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@45355762)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@609f4c02)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@3f62cd45)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@90a8c93)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@64f86431)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7d21a705)\n",
       "\t- field (class: $line24.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line24.$read, $line24.$read@78b2d7b4)\n",
       "\t- field (class: $iw, name: $line24$read, type: class $line24.$read)\n",
       "\t- object (class $iw, $iw@3cdffe37)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4004afa9)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@161cf467)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4bfacbb6)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@682c9a1b)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@392c221c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@78682e5d)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@48f869ce)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@714e9508)\n",
       "\t- field (class: $line107.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line107.$read, $line107.$read@786e20f1)\n",
       "\t- field (class: $iw, name: $line107$read, type: class $line107.$read)\n",
       "\t- object (class $iw, $iw@2fdea881)\n",
       "\t- field (class: $iw, name: $outer, type: class $iw)\n",
       "\t- object (class $iw, $iw@4688b4)\n",
       "\t- field (class: $anonfun$1, name: $outer, type: class $iw)\n",
       "\t- object (class $anonfun$1, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var averageMap = flattenContinuousData.map{x => count}\n",
    "averageMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)\n",
       "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2287)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.map(RDD.scala:369)\n",
       "  ... 46 elided\n",
       "Caused by: java.io.NotSerializableException: Common$\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: Common$, value: Common$@146b6344)\n",
       "\t- field (class: $iw, name: Common$module, type: class Common$)\n",
       "\t- object (class $iw, $iw@75d4b24f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4da1638)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1855ae81)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5fcc5d47)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@623b8339)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@164480e1)\n",
       "\t- field (class: $line13.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line13.$read, $line13.$read@30056fee)\n",
       "\t- field (class: $iw, name: $line13$read, type: class $line13.$read)\n",
       "\t- object (class $iw, $iw@5cc6c0e)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@45355762)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@609f4c02)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@3f62cd45)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@90a8c93)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@64f86431)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7d21a705)\n",
       "\t- field (class: $line24.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line24.$read, $line24.$read@78b2d7b4)\n",
       "\t- field (class: $iw, name: $line24$read, type: class $line24.$read)\n",
       "\t- object (class $iw, $iw@3cdffe37)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4004afa9)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@161cf467)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4bfacbb6)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@682c9a1b)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@392c221c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@78682e5d)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@48f869ce)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@714e9508)\n",
       "\t- field (class: $line107.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line107.$read, $line107.$read@786e20f1)\n",
       "\t- field (class: $iw, name: $line107$read, type: class $line107.$read)\n",
       "\t- object (class $iw, $iw@7f8d694c)\n",
       "\t- field (class: $iw, name: $outer, type: class $iw)\n",
       "\t- object (class $iw, $iw@59e3ad74)\n",
       "\t- field (class: $anonfun$2, name: $outer, type: class $iw)\n",
       "\t- object (class $anonfun$2, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var averageMap = flattenContinuousData.reduceByKey((i, j) => i+j).map{case (k, v) => count.value(\"dst_bytes\")}.collect()\n",
    "Common.printSample(writer, averageMap, \"Average Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val std = flattenContinuousData.\n",
    "    map{case (k,v) => (k, scala.math.pow(v-average(k),2))}.\n",
    "    reduceByKey((i,j) => i+j).\n",
    "    map{case (k,v) => (k, math.sqrt(v/count(k)))}.collectAsMap()\n",
    "Common.printSample(writer, std, \"Standard Deviation Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Find Min, Max and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(Global Active Power -> 2049280, Global Reactive Power -> 2049280, Global Intensity -> 2049280, Voltage -> 2049280)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val count = flattenData.map{case (k,v) => (k,1)}.reduceByKey((i, j) => i+j).collectAsMap()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(Global Active Power -> 11.122, Global Reactive Power -> 1.39, Global Intensity -> 48.4, Voltage -> 254.15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val max = flattenData.reduceByKey{(i, j) => if (i>j) i else j}.collectAsMap()\n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(Global Active Power -> 0.076, Global Reactive Power -> 0.0, Global Intensity -> 0.2, Voltage -> 223.2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val min = flattenData.reduceByKey{(i, j) => if (i<j) i else j}.collectAsMap()\n",
    "min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Mean & Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(Global Active Power -> 1.0916150365005446, Global Reactive Power -> 0.12371447630388221, Global Intensity -> 4.627759310588324, Voltage -> 240.8398579745135)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val average = flattenData.reduceByKey((i, j) => i+j).map{case (i, j) => (i,j/2049280)}.collectAsMap()\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(Global Active Power -> 1.0572939031266613, Global Reactive Power -> 0.11272195204783488, Global Intensity -> 4.444395175407247, Voltage -> 3.239985888491343)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val std = flattenData.\n",
    "    map{case (k,v) => (k, scala.math.pow(v-average(k),2))}.\n",
    "    reduceByKey((i,j) => i+j).\n",
    "    map{case (k,v) => (k, math.sqrt(v/count(k)))}.collectAsMap()\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Min-Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((Global Active Power,0.3747963063552418), (Global Reactive Power,0.30071942446043165), (Voltage,0.376090468497577), (Global Intensity,0.37759336099585067), (Global Active Power,0.4783632084012313), (Global Reactive Power,0.31366906474820144), (Voltage,0.33699515347334413), (Global Intensity,0.47302904564315357), (Global Active Power,0.4796306355241717), (Global Reactive Power,0.35827338129496406))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val norm = flattenData.take(10).map{case (k,v) => (k, (v-min(k))/(max(k)-min(k)))}\n",
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Global Active Power:\n",
      "        Number of Meaningful Data - 2049280\n",
      "        Maximum Value - 11.122\n",
      "        Minimum Value - 0.076\n",
      "        Mean - 1.0916150365005446\n",
      "        Standard Deviation - 1.0572939031266613\n",
      "\n",
      "For Global Reactive Power:\n",
      "        Number of Meaningful Data - 2049280\n",
      "        Maximum Value - 1.39\n",
      "        Minimum Value - 0.0\n",
      "        Mean - 0.12371447630388221\n",
      "        Standard Deviation - 0.11272195204783488\n",
      "\n",
      "For Voltage:\n",
      "        Number of Meaningful Data - 2049280\n",
      "        Maximum Value - 254.15\n",
      "        Minimum Value - 223.2\n",
      "        Mean - 240.8398579745135\n",
      "        Standard Deviation - 3.239985888491343\n",
      "\n",
      "For Global Intensity:\n",
      "        Number of Meaningful Data - 2049280\n",
      "        Maximum Value - 48.4\n",
      "        Minimum Value - 0.2\n",
      "        Mean - 4.627759310588324\n",
      "        Standard Deviation - 4.444395175407247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "object MyFunctions {\n",
    "    def myprint(s: String): Unit = {\n",
    "        println(\"For \"+s+\":\")\n",
    "        println(\"        Number of Meaningful Data - \" + count(s))\n",
    "        println(\"        Maximum Value - \" + max(s))\n",
    "        println(\"        Minimum Value - \" + min(s))\n",
    "        println(\"        Mean - \" + average(s))\n",
    "        println(\"        Standard Deviation - \" + std(s) + \"\\n\")\n",
    "    }    \n",
    "}\n",
    "\n",
    "MyFunctions.myprint(\"Global Active Power\")\n",
    "MyFunctions.myprint(\"Global Reactive Power\")\n",
    "MyFunctions.myprint(\"Voltage\")\n",
    "MyFunctions.myprint(\"Global Intensity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "val spark2 = spark\n",
    "import spark2.implicits._\n",
    "\n",
    "object Norm {\n",
    "    def AP(s: String): Double = {\n",
    "        process(s, \"Global Active Power\")\n",
    "    } \n",
    "    \n",
    "    def RP(s: String): Double = {\n",
    "        process(s, \"Global Reactive Power\")\n",
    "    } \n",
    "    \n",
    "    def V(s: String): Double = {\n",
    "        process(s, \"Voltage\")\n",
    "    } \n",
    "    \n",
    "    def I(s: String): Double = {\n",
    "        process(s, \"Global Intensity\")\n",
    "    } \n",
    "    \n",
    "    def process(s: String, k: String): Double = {\n",
    "        var retVal = 0.0\n",
    "        if(s == \"?\"){\n",
    "            retVal = Double.NaN\n",
    "        }else{\n",
    "            retVal = (s.toDouble-min(k))/(max(k)-min(k))\n",
    "        }\n",
    "        retVal\n",
    "    }\n",
    "}\n",
    "\n",
    "val dataDF = rows.\n",
    "                map(_.split(\";\")).\n",
    "                map(att => (att(0), att(1), Norm.AP(att(2)), Norm.RP(att(3)), Norm.V(att(4)), Norm.I(att(5)) )).\n",
    "                toDF(\"Date\", \"Time\", \"Active_Power\", \"Reactive_Power\",\"Voltage\",\"Intensity\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.write.csv(\"./data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+-------------------+-------------------+--------------------+\n",
      "|     Date|    Time|        Active_Power|     Reactive_Power|            Voltage|           Intensity|\n",
      "+---------+--------+--------------------+-------------------+-------------------+--------------------+\n",
      "|28/4/2007|00:00:00| 0.11696541734564549| 0.0618705035971223|0.31825525040387775| 0.11618257261410789|\n",
      "|28/4/2007|00:01:00|  0.1171464783632084| 0.0618705035971223|0.32374798061389354| 0.11618257261410789|\n",
      "|28/4/2007|00:02:00| 0.11732753938077133|0.06330935251798561| 0.3350565428109854| 0.11618257261410789|\n",
      "|28/4/2007|00:03:00|  0.1171464783632084| 0.0618705035971223| 0.3295638126009697| 0.11618257261410789|\n",
      "|28/4/2007|00:04:00| 0.11696541734564549| 0.0618705035971223| 0.3247172859450729| 0.11618257261410789|\n",
      "|28/4/2007|00:05:00| 0.11696541734564549| 0.0618705035971223|0.32213247172859427| 0.11618257261410789|\n",
      "|28/4/2007|00:06:00|  0.1171464783632084| 0.0618705035971223|0.32859450726979034| 0.11618257261410789|\n",
      "|28/4/2007|00:07:00| 0.11642223429295674|  0.060431654676259| 0.3021001615508891| 0.11618257261410789|\n",
      "|28/4/2007|00:08:00| 0.11642223429295674|  0.060431654676259| 0.2982229402261717| 0.11618257261410789|\n",
      "|28/4/2007|00:09:00| 0.11678435632808257| 0.0618705035971223| 0.3185783521809373| 0.11618257261410789|\n",
      "|28/4/2007|00:10:00| 0.11696541734564549| 0.0618705035971223|0.32084006462035547| 0.11618257261410789|\n",
      "|28/4/2007|00:11:00|  0.1171464783632084| 0.0618705035971223|0.33085621970920853| 0.11618257261410789|\n",
      "|28/4/2007|00:12:00|   0.126923773311606|0.14244604316546763| 0.3353796445880458|  0.1286307053941909|\n",
      "|28/4/2007|00:13:00| 0.11950027159152633| 0.1510791366906475| 0.3421647819063004| 0.12033195020746888|\n",
      "|28/4/2007|00:14:00|0.037298569617961255|0.15683453237410072| 0.3744749596122778|0.041493775933609964|\n",
      "|28/4/2007|00:15:00| 0.03766069165308709|0.13093525179856116|0.40516962843295684|0.041493775933609964|\n",
      "|28/4/2007|00:16:00|0.038203874705775846|0.14676258992805755| 0.4029079159935377|0.041493775933609964|\n",
      "|28/4/2007|00:17:00| 0.03802281368821293|0.14676258992805755|0.40161550888529884|0.041493775933609964|\n",
      "|28/4/2007|00:18:00| 0.03784175267065001|0.14676258992805755| 0.4009693053311799|0.041493775933609964|\n",
      "|28/4/2007|00:19:00|0.037479630635524175|0.14532374100719425| 0.3819063004846531|0.041493775933609964|\n",
      "|28/4/2007|00:20:00| 0.03766069165308709|0.14964028776978416| 0.4213247172859455|0.041493775933609964|\n",
      "|28/4/2007|00:21:00|                 NaN|                NaN|                NaN|                 NaN|\n",
      "|28/4/2007|00:22:00|                 NaN|                NaN|                NaN|                 NaN|\n",
      "|28/4/2007|00:23:00|                 NaN|                NaN|                NaN|                 NaN|\n",
      "|28/4/2007|00:24:00|                 NaN|                NaN|                NaN|                 NaN|\n",
      "+---------+--------+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.createOrReplaceTempView(\"records\")\n",
    "spark.sql(\"SELECT * FROM records WHERE date = '28/4/2007'\").show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
