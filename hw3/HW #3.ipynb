{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "[Reuters-21578 Text Categorization Collection Data Set](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection) - about 2 million instances, 28MB in size\n",
    "\n",
    "\n",
    "News: 21 SGML files\n",
    "We only deal with news contents inside <body> </body> tags\n",
    "The other files will not be needed in this homework\n",
    "\n",
    "### Format\n",
    "One text file consisting of lines of records.\n",
    "\n",
    "Each record contains 9 attributes separated by semicolons: \n",
    "\n",
    "\n",
    "Reuters-21578, Distribution 1.0 includes five files (all-exchanges-strings.lc.txt, all-orgs-strings.lc.txt, all-people-strings.lc.txt, all-places-strings.lc.txt, and all-topics-strings.lc.txt) which list the names of *all* legal categories in each set. A sixth file, cat-descriptions_120396.txt gives some additional information on the category sets.\n",
    "\n",
    "\n",
    "### Task\n",
    "4 subtasks:\n",
    "+ (30pt) Given the Reuters-21578 dataset, please calculate all k-shingles and output the set representation of the text dataset as a matrix.\n",
    "+ (30pt) Given the set representation, compute the minhash signatures of all documents using MapReduce.\n",
    "+ (40pt) Implement the LSH algorithm by MapReduce and output the resulting candidate pairs of similar documents. \n",
    "+ (Bonus) Implement K-nearest neighbor (KNN) search using LSH and compare its performance with linear search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Format\n",
    "\n",
    "1. Set representation:\n",
    "A MxN matrix: with rows as shingles and columns as documents (N=21,578) \n",
    "2. minhash signatures:\n",
    "The HxN signature matrix: with H as the number of hash functions, N=21,578\n",
    "3. candidate pairs:\n",
    "For each document i, there should be a list of those documents j>i with which i needs to be compared\n",
    "4. comparison of KNN search an linear search\n",
    "\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "Note the differences in rows and columns for the input data and the output matrices\n",
    "* Input: rows as documents\n",
    "* Output: columns as documents\n",
    "\n",
    "Your program should be able to accept some parameters:\n",
    "* k in k-shingles\n",
    "* Number of hash functions H\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@3bac3c23\n",
      "Spark version:      2.2.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "printSample: (writer: Any, data: Any, title: String, format: String)Unit\n",
       "printSpark: (writer: Any, spark: org.apache.spark.sql.SparkSession)Unit\n",
       "outputWriter: (fileString: String)java.io.PrintWriter\n",
       "getFile: (fileString: String)Array[String]\n",
       "writer: Null = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.hadoop.fs._\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import java.io.{File,PrintWriter}\n",
    "\n",
    "  def printSample(writer: Any, data: Any, title: String = \"\", format: String = \"\"){\n",
    "    println(\"\\n\"+title+\" Data Sample: \" + format)\n",
    "    println(data+\"\\n\")\n",
    "  }\n",
    "\n",
    "  def printSpark(writer: Any, spark: SparkSession): Unit = {\n",
    "    println(\"Spark Entity:       \" + spark)\n",
    "    println(\"Spark version:      \" + spark.version)\n",
    "    println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "    println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)\n",
    "    println(\"\")\n",
    "  }\n",
    "\n",
    "  def outputWriter(fileString: String): PrintWriter ={\n",
    "    val outputPath = new Path(fileString)\n",
    "    val outputStream = outputPath.getFileSystem(new Configuration()).create(outputPath);\n",
    "    new PrintWriter(outputStream)\n",
    "  }\n",
    "\n",
    "  def getFile(fileString: String): Array[String] ={\n",
    "    val inputPath = new Path(fileString)\n",
    "    val inputBuffer = scala.collection.mutable.ArrayBuffer.empty[String]\n",
    "    val iterator = inputPath.getFileSystem(new Configuration()).listFiles(inputPath, false)\n",
    "    while(iterator.hasNext()){\n",
    "        val fileStatus = iterator.next()\n",
    "        if(fileStatus.isFile()){\n",
    "          inputBuffer += fileStatus.getPath().toString()\n",
    "        }\n",
    "    }\n",
    "    inputBuffer.toArray\n",
    "  }\n",
    "\n",
    "\n",
    "val writer = null\n",
    "printSpark(writer, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@3bac3c23\n",
       "data = Array(file:/home/micky/big_data/hw3/data/reut2-017.sgm, file:/home/micky/big_data/hw3/data/reut2-001.sgm, file:/home/micky/big_data/hw3/data/reut2-021.sgm, file:/home/micky/big_data/hw3/data/reut2-003.sgm, file:/home/micky/big_data/hw3/data/reut2-010.sgm, file:/home/micky/big_data/hw3/data/reut2-013.sgm, file:/home/micky/big_data/hw3/data/reut2-004.sgm, file:/home/micky/big_data/hw3/data/reut2-020.sgm, file:/home/micky/big_data/hw3/data/reut2-008.sgm, file:/home/micky/big_data/hw3/data/reut2-014.sgm, file:/home/micky/big_data/hw3/data/reut2-007.sgm, file:/home/micky/big_data/hw3/data/reut2-018.sgm, file:/home/micky/big_data/hw3/data/reut2-005.sgm, file:/home/micky/big_data/hw3/da...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[file:/home/micky/big_data/hw3/data/reut2-017.sgm, file:/home/micky/big_data/hw3/data/reut2-001.sgm, file:/home/micky/big_data/hw3/data/reut2-021.sgm, file:/home/micky/big_data/hw3/data/reut2-003.sgm, file:/home/micky/big_data/hw3/data/reut2-010.sgm, file:/home/micky/big_data/hw3/data/reut2-013.sgm, file:/home/micky/big_data/hw3/data/reut2-004.sgm, file:/home/micky/big_data/hw3/data/reut2-020.sgm, file:/home/micky/big_data/hw3/data/reut2-008.sgm, file:/home/micky/big_data/hw3/data/reut2-014.sgm, file:/home/micky/big_data/hw3/data/reut2-007.sgm, file:/home/micky/big_data/hw3/data/reut2-018.sgm, file:/home/micky/big_data/hw3/data/reut2-005.sgm, file:/home/micky/big_data/hw3/data/reut2-015.sgm, file:/home/micky/big_data/hw3/data/reut2-011.sgm, file:/home/micky/big_data/hw3/data/reut2-002.sgm, file:/home/micky/big_data/hw3/data/reut2-012.sgm, file:/home/micky/big_data/hw3/data/reut2-019.sgm, file:/home/micky/big_data/hw3/data/reut2-009.sgm, file:/home/micky/big_data/hw3/data/reut2-016.sgm, file:/home/micky/big_data/hw3/data/reut2-000.sgm, file:/home/micky/big_data/hw3/data/reut2-006.sgm]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark2 = spark\n",
    "\n",
    "var data = getFile(\"./data\")\n",
    "var sample_data = Array(\"file:/home/micky/big_data/hw3/data/reut2-017.sgm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count: 940\n",
      "\n",
      "Parsed Sample Data Sample: \n",
      "(17001,The Brazilian Coffee Institute, IBC,\n",
      "plans to sell in a series of auctions over the next few weeks\n",
      "robusta coffee purchased in London last year, but details of\n",
      "where and when auctions will take place are still to be\n",
      "finalised, IBC president Jorio Dauster told reporters.\n",
      "    The sales of 630,000 bags of robusta and an unspecified\n",
      "amount of Brazilian arabica coffee will take place over a\n",
      "minimum of six months but it is not decided where sales will\n",
      "take place or whether they will be held weekly or monthly.\n",
      "    The amount offered at each sale has also not been set, but\n",
      "could be in the order of 100,000 bags, Dauster said.)\n",
      "\n",
      "(17002,AMCA International Ltd said it\n",
      "appointed president and chief executive officer WIlliam Holland\n",
      "to succeed Kenneth Barclay as chairman.\n",
      "    Barclay, who is 60 years old, decided not to stand for\n",
      "reappointment as chairman this year but will continue as a\n",
      "director, AMCA said.)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "regex = (?s)<REUTERS.*?NEWID=\"(\\d*)\".*?>.*?<TEXT.*?>(.*?)<\\/TEXT>.*?<\\/REUTERS>\n",
       "body_regex = (?s).*?<BODY.*?>(.*?)<\\/BODY>.*?\n",
       "newsParse = UnionRDD[4] at union at <console>:59\n",
       "newsCount = 940\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "//TODO: Sample -> Actual\n",
    "var regex = \"\"\"(?s)<REUTERS.*?NEWID=\"(\\d*)\".*?>.*?<TEXT.*?>(.*?)<\\/TEXT>.*?<\\/REUTERS>\"\"\".r\n",
    "var body_regex = \"\"\"(?s).*?<BODY.*?>(.*?)<\\/BODY>.*?\"\"\".r\n",
    "var newsParse = spark.sparkContext.emptyRDD[(Int, String)]\n",
    "\n",
    "sample_data.map{\n",
    "    path  =>\n",
    "    val singleNewsParse = spark.sparkContext.wholeTextFiles(path).flatMap{\n",
    "        case (_, string) =>\n",
    "        val list =  ListBuffer[Tuple2[Int, String]]()\n",
    "        for(m <- regex.findAllIn(string).matchData){\n",
    "            for( n <- body_regex.findAllIn(m.group(2)).matchData){\n",
    "                list += (Tuple2(m.group(1).toInt, n.group(1).replace(\"\\n Reuter\\n&#3;\",\"\")))\n",
    "            }\n",
    "        }\n",
    "        list.toSeq\n",
    "    }\n",
    "    newsParse = newsParse.union(singleNewsParse)\n",
    "}\n",
    "var newsCount = newsParse.count()\n",
    "print(\"\\nCount: \"+ newsCount+\"\\n\")\n",
    "printSample(writer, newsParse.take(2).mkString(\"\\n\\n\"), \"Parsed Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Calculate K-Shingle Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPreProcess: (input: Any)Array[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wordPreProcess(input: Any): Array[String] = {\n",
    "        var matchRegex = \"\"\"([$]?(?:[\\w]+(?:[\\w',]*[\\w]+)+|[\\w]))\"\"\".r\n",
    "        var list =  ListBuffer[String]()\n",
    "        for(m <- matchRegex.findAllIn(input.toString.toLowerCase).matchData;\n",
    "          e <- m.subgroups)\n",
    "          list+=e\n",
    "        list.toArray\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Shingle Sample Data Sample: \n",
      "(the brazilian coffee,17001)\n",
      "(brazilian coffee institute,17001)\n",
      "(coffee institute ibc,17001)\n",
      "(institute ibc plans,17001)\n",
      "(ibc plans to,17001)\n",
      "(plans to sell,17001)\n",
      "(to sell in,17001)\n",
      "(sell in a,17001)\n",
      "(in a series,17001)\n",
      "(a series of,17001)\n",
      "(series of auctions,17001)\n",
      "(of auctions over,17001)\n",
      "(auctions over the,17001)\n",
      "(over the next,17001)\n",
      "(the next few,17001)\n",
      "(next few weeks,17001)\n",
      "(few weeks robusta,17001)\n",
      "(weeks robusta coffee,17001)\n",
      "(robusta coffee purchased,17001)\n",
      "(coffee purchased in,17001)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kShingleValue = 3\n",
       "parseSentenceRegex = ([$]?(?:[\\w]+(?:[\\w',]*[\\w]+)+|[\\w]))\n",
       "flattenShingles = MapPartitionsRDD[5] at flatMap at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at flatMap at <console>:46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kShingleValue = 3\n",
    "val parseSentenceRegex = \"\"\"([$]?(?:[\\w]+(?:[\\w',]*[\\w]+)+|[\\w]))\"\"\".r\n",
    "\n",
    "var flattenShingles = newsParse.flatMap{\n",
    "    case (id, string)  =>\n",
    "    wordPreProcess(string).sliding(kShingleValue).map{\n",
    "        wordVector =>\n",
    "        (wordVector.mkString(\" \"), id)\n",
    "    }  \n",
    "}\n",
    "printSample(writer, flattenShingles.take(20).mkString(\"\\n\"), \"Parsed Shingle Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ziped With Index Sample Data Sample: \n",
      "17758 -> 719\n",
      "17417 -> 411\n",
      "17408 -> 402\n",
      "17004 -> 3\n",
      "17749 -> 711\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrixIndex = Broadcast(6)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var matrixIndex = spark.sparkContext.broadcast(newsParse.map{\n",
    "    case(id, string) =>\n",
    "    id.toInt\n",
    "}.sortBy{\n",
    "    case (id) => id\n",
    "}.zipWithIndex.collectAsMap())\n",
    "\n",
    "printSample(writer, matrixIndex.value.take(5).mkString(\"\\n\"), \"Ziped With Index Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouped Shingle Sample Data Sample: \n",
      "(attacks on the,CompactBuffer(17406))\n",
      "(he said and,CompactBuffer(17166, 17760, 17915))\n",
      "(more than four,CompactBuffer(17422, 17644, 17871))\n",
      "(improving high quality,CompactBuffer(17474))\n",
      "(expire at end,CompactBuffer(17043))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedShingles = ShuffledRDD[11] at groupByKey at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[11] at groupByKey at <console>:46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var groupedShingles = flattenShingles.groupByKey()\n",
    "\n",
    "printSample(writer, groupedShingles.take(5).mkString(\"\\n\"), \"Grouped Shingle Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 128427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newsCount = Broadcast(10)\n",
       "shinglesCount = Broadcast(12)\n",
       "shingleMatrix = MapPartitionsRDD[12] at map at <console>:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at map at <console>:52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var newsCount = spark.sparkContext.broadcast(newsParse.count())\n",
    "var shinglesCount = spark.sparkContext.broadcast(groupedShingles.count())\n",
    "print(\"Count: \"+shinglesCount.value+\"\\n\")\n",
    "\n",
    "var shingleMatrix = groupedShingles.map{\n",
    "    case (text, iterable) =>\n",
    "    var row = Array.fill(newsCount.value.toInt){false}\n",
    "    iterable.toVector.map{\n",
    "        id =>\n",
    "        row(matrixIndex.value(id).toInt) = true\n",
    "    }\n",
    "    row\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val shingleString = shingleMatrix.map{\n",
    "      array =>\n",
    "      array.map{bool => if(bool) '1' else '0'}.mkString(\",\")\n",
    "}\n",
    "\n",
    "shingleString.saveAsTextFile(\"./output/shingleMatrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 2 - Compute Min-Hash Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numOfHashFunctions = Broadcast(13)\n",
       "prime = 21601\n",
       "rand = scala.util.Random$@20b2fb24\n",
       "hashFunctionMatrix = Array(Array(466, -647), Array(-167, -972), Array(-36, -429), Array(-328, 8), Array(49, -884), Array(737, -938), Array(-813, 808), Array(439, 751), Array(705, -694), Array(115, 8), Array(-999, 532), Array(719, -749), Array(-649, -218), Array(-131, -666), Array(604, -933), Array(257, -934), Array(890, 108), Array(-896, -815), Array(540, 751), Array(-768, 821), Array(994, 448), Array(126, -247), Array(-103, 602), Array(591, -321), Array(297, -687), Array(206, 310), Array(233, -289), Array(500, -548), Array(733, 826), Array(-938, 952), Array(271, 175), Array(24, -787), Array(-4, -755), Array(944, -452), ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[466, -647], [-167, -972], [-36, -429], [-328, 8], [49, -884], [737, -938], [-813, 808], [439, 751], [705, -694], [115, 8], [-999, 532], [719, -749], [-649, -218], [-131, -666], [604, -933], [257, -934], [890, 108], [-896, -815], [540, 751], [-768, 821], [994, 448], [126, -247], [-103, 602], [591, -321], [297, -687], [206, 310], [233, -289], [500, -548], [733, 826], [-938, 952], [271, 175], [24, -787], [-4, -755], [944, -452], [-203, 256], [517, 985], [597, -728], [-779, -169], [-381, 277], [518, 867], [-51, 481], [-524, 794], [548, 123], [937, -632], [-745, 811], [75, 14], [200, -217], [688, -801], [746, 753], [60, -969], [956, -470], [11, 57], [-561, -776], [-934, 915], [51, 559], [-273, -176], [-608, 398], [-439, -486], [144, -343], [849, 409], [506, -769], [-224, -115], [-519, -217], [91, 932], [555, 892], [-214, 520], [549, -462], [838, 440], [-892, -685], [-992, 225], [846, -37], [-34, 713], [-393, 364], [768, -40], [768, -209], [-383, 836], [-247, 120], [-679, 999], [-249, 804], [-545, 495], [81, 930], [-180, 834], [660, -165], [667, -933], [407, -545], [-68, -987], [376, -642], [647, -989], [-187, 15], [366, -934], [-745, -112], [460, 556], [-567, 235], [233, 146], [678, -654], [-929, 741], [816, -473], [391, 521], [-563, 922], [396, -682]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var numOfHashFunctions = spark.sparkContext.broadcast(100)\n",
    "var prime = 21601\n",
    "\n",
    "val rand = scala.util.Random\n",
    "var hashFunctionMatrix = Array.fill(numOfHashFunctions.value,2){rand.nextInt(2000) - 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashFunction: (input: Int)Array[Int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hashFunction(input: Int): Array[Int] = {\n",
    "//     var hashResultMatrix = Array.ofDim[String](numOfHashFunctions)\n",
    "    hashFunctionMatrix.map{\n",
    "        array =>\n",
    "        (Math.abs(array(0).toInt * input + array(1).toInt)%prime)%newsCount.value.toInt\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val signatureMatrix = shingleMatrix.zipWithIndex.mapPartitions{\n",
    "    itr =>\n",
    "    \n",
    "    itr.map{\n",
    "        case(rowArray, index) =>\n",
    "        var matrix = Array.fill(numOfHashFunctions.value,newsCount.value.toInt){99999}\n",
    "        var hashResult = hashFunction(index.toInt).toSeq\n",
    "        rowArray.zipWithIndex.map{case (x,i) =>\n",
    "            if(x!=false){\n",
    "                for(n <- 0 until numOfHashFunctions.value){\n",
    "                    matrix(n)(i) = hashResult(n)\n",
    "                }\n",
    "            }\n",
    "        }      \n",
    "        matrix\n",
    "    }\n",
    "\n",
    "}.reduce{\n",
    "    case (matrix1, matrix2) =>\n",
    "    for(y <- 0 until matrix1.length;\n",
    "      x <- 0 until matrix1(y).length){\n",
    "        if(matrix1(y)(x) > matrix2(y)(x)){\n",
    "            matrix1(y)(x) = matrix2(y)(x)\n",
    "        }\n",
    "    }\n",
    "    matrix1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val signatureString = signatureMatrix.map{\n",
    "      array =>\n",
    "      array.mkString(\",\")\n",
    "}.mkString(\"\\n\")\n",
    "\n",
    "val local_writer =  outputWriter(\"./output/signatureMatrix\")\n",
    "try {local_writer.write(signatureString)} finally {local_writer.close()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val hashFunctionString = hashFunctionMatrix.map{\n",
    "      array =>\n",
    "      array.mkString(\",\")\n",
    "}.mkString(\"\\n\")\n",
    "\n",
    "val local_writer =  outputWriter(\"./output/hashFunctionMatrix\")\n",
    "try {local_writer.write(hashFunctionString)} finally {local_writer.close()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Implement LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowsPerBand = Broadcast(15)\n",
       "jaccardSimilarity = Broadcast(16)\n",
       "bandMax = 20.0\n",
       "bandIndexMap = ParallelCollectionRDD[15] at parallelize at <console>:62\n",
       "signatureBroadcast = Broadcast(17)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(17)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rowsPerBand = spark.sparkContext.broadcast(5)\n",
    "var jaccardSimilarity = spark.sparkContext.broadcast(0.8)\n",
    "var bandMax = Math.ceil(numOfHashFunctions.value/rowsPerBand.value)\n",
    "var bandIndexMap = spark.sparkContext.parallelize((0 until bandMax.toInt).toList)\n",
    "var signatureBroadcast = spark.sparkContext.broadcast(signatureMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var LSH_matrix = bandIndexMap.map{\n",
    "    n =>\n",
    "    var matrix = Array.fill(newsCount.value.toInt,newsCount.value.toInt){false}\n",
    "    for(y <- 0 until newsCount.value.toInt;\n",
    "      x <- (y + 1) until newsCount.value.toInt){\n",
    "        var matchCounter = 0;\n",
    "        for(count <- 0 until rowsPerBand.value.toInt){\n",
    "            if(signatureBroadcast.value(n * rowsPerBand.value.toInt + count)(y) == signatureBroadcast.value(n * rowsPerBand.value.toInt + count)(x)){\n",
    "                matchCounter += 1;\n",
    "            }\n",
    "        }\n",
    "        if(matchCounter.toDouble/rowsPerBand.value >= jaccardSimilarity.value){\n",
    "            matrix(y)(x) = true;\n",
    "            matrix(x)(y) = true;\n",
    "        }\n",
    "    }\n",
    "    matrix\n",
    "}.reduce{\n",
    "    case (matrix1, matrix2) =>\n",
    "    for(y <- 0 until matrix1.length;\n",
    "      x <- 0 until matrix1(y).length){\n",
    "        if(matrix2(y)(x)){\n",
    "            matrix1(y)(x) = true\n",
    "        }\n",
    "    }\n",
    "    matrix1\n",
    "}\n",
    "\n",
    "LSH_matrix(0).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var LSH_pairs = LSH_matrix.map{\n",
    "    array =>\n",
    "    var list =  ListBuffer[Int]()\n",
    "    array.zipWithIndex.map{\n",
    "        case (v, i) =>\n",
    "        if(v==true){\n",
    "            list+=i\n",
    "        }        \n",
    "    }\n",
    "    list.toSeq\n",
    "}\n",
    "println(LSH_pairs(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val LSH_String = LSH_pairs.zipWithIndex.map{\n",
    "      case(array, i) =>\n",
    "      (i+1) + \": \" + array.mkString(\",\")\n",
    "}.mkString(\"\\n\")\n",
    "\n",
    "val local_writer =  outputWriter(\"./output/LSH_Pairs\")\n",
    "try {local_writer.write(LSH_String)} finally {local_writer.close()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Comparing LSH vs Linear Search Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val groupedFlattenShingles = flattenShingles.map{\n",
    "    case (shingle, id) =>\n",
    "    ( matrixIndex.value(id).toInt, shingle)\n",
    "}.groupByKey()\n",
    "\n",
    "val mapFlattenShingles = groupedFlattenShingles.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH Duration: 4.0371E-5 secs\n",
      "Linear Duration: 1.331535873 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "getDocID = 3\n",
       "t1 = 4818461902939\n",
       "LSH_Reslut_GetDoc = List(22, 26, 35, 50, 99, 168, 169, 197, 200, 214, 250, 277, 295, 301, 333, 351, 362, 366, 380, 388, 410, 427, 444, 466, 476, 490, 500, 518, 609, 662, 721, 729, 737, 798, 811, 860, 868, 875, 903)\n",
       "duration = 4.0371E-5\n",
       "t2 = 4818462818423\n",
       "docShingle = Broadcast(53)\n",
       "LinearResult = Array(24, 111, 141, 173, 177, 551, 611, 622, 735, 742, 743, 756, 789)\n",
       "duration2 = 1.331535873\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.331535873"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val getDocID = 3;\n",
    "\n",
    "var t1 = System.nanoTime\n",
    "\n",
    "var LSH_Reslut_GetDoc = LSH_pairs(getDocID)\n",
    "\n",
    "var duration = (System.nanoTime - t1) / 1e9d\n",
    "println(\"LSH Duration: \"+duration+\" secs\")\n",
    "\n",
    "var t2 = System.nanoTime\n",
    "\n",
    "val docShingle = spark.sparkContext.broadcast(mapFlattenShingles(getDocID).toVector)\n",
    "\n",
    "var LinearResult = groupedFlattenShingles.map{\n",
    "    case(id, buffer) =>\n",
    "    var returnValue = -1\n",
    "    if(id != getDocID){\n",
    "      val compShingle = buffer.toVector \n",
    "      if(docShingle.value.intersect(compShingle).size.toDouble / docShingle.value.union(compShingle).size.toDouble >= 0.01){\n",
    "          returnValue = id\n",
    "      }\n",
    "    }\n",
    "    returnValue\n",
    "}.filter(x => x!= -1).sortBy(x => x).collect()\n",
    "\n",
    "var duration2 = (System.nanoTime - t1) / 1e9d\n",
    "println(\"Linear Duration: \"+duration2+\" secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
