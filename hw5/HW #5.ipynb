{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "[Google web graph dataset](http://snap.stanford.edu/data/web-Google.html) - about 5 million edwges, 20MB (compressed) in size\n",
    "\n",
    "Nodes represent web pages and directed edges represent hyperlinks between them. \n",
    "The data was released in 2002 by Google as a part of Google Programming Contest.\n",
    "\n",
    "### Format\n",
    "One text file consisting of lines of records.\n",
    "\n",
    "Each line is a directed edge representing hyperlinks between nodes (web pages) </br>\n",
    "Example: <FromNodeID\\> <ToNodeID\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "3 subtasks:\n",
    "+ (30pt) Given the Google web graph dataset, please output the list of web pages with the number of outlinks, sorted in descending order of the out-degrees.\n",
    "+ (30pt) Please output the inlink distribution of the top linked web pages, sorted in descending order of the in-degrees.\n",
    "+ (40pt) Design an algorithm that maintains the connectivity of two nodes in an efficient way. Given a node v, please output the list of nodes that v points to, and the list of nodes that points to v.\n",
    "+ (Bonus) Compute the PageRank of the graph using MapReduce.\n",
    "\n",
    "### Output Format\n",
    "1. a sorted list of pages with their out-degrees\n",
    "    + Each line contains: <NodeID\\>, <out-degree\\>\n",
    "1. a sorted list of pages with their in-degrees\n",
    "    + Each line contains: <NodeID\\>, <in-degree\\>\n",
    "1. Given a node v,\n",
    "    + The first line contains a list of nodes that v points to:\n",
    "        + <ToNodeID\\>, …, <ToNodeID\\>\n",
    "    + The second line contains a list of nodes point to v\n",
    "        + <FromNodeID\\>, …, <FromNodeID\\>\n",
    "1. The PageRank of each node in steady state\n",
    "    + Each line contains: <NodeID\\>, <Score\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@1428ad24\n",
      "Spark version:      2.3.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "printSample: (writer: Any, data: Any, title: String, format: String)Unit\n",
       "printSpark: (writer: Any, spark: org.apache.spark.sql.SparkSession)Unit\n",
       "outputWriter: (fileString: String)java.io.PrintWriter\n",
       "getFile: (fileString: String)Array[String]\n",
       "writer: Null = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.hadoop.fs._\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import java.io.{File,PrintWriter}\n",
    "\n",
    "  def printSample(writer: Any, data: Any, title: String = \"\", format: String = \"\"){\n",
    "    println(\"\\n\"+title+\" Data Sample: \" + format)\n",
    "    println(data+\"\\n\")\n",
    "  }\n",
    "\n",
    "  def printSpark(writer: Any, spark: SparkSession): Unit = {\n",
    "    println(\"Spark Entity:       \" + spark)\n",
    "    println(\"Spark version:      \" + spark.version)\n",
    "    println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "    println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)\n",
    "    println(\"\")\n",
    "  }\n",
    "\n",
    "  def outputWriter(fileString: String): PrintWriter ={\n",
    "    val outputPath = new Path(fileString)\n",
    "    val outputStream = outputPath.getFileSystem(new Configuration()).create(outputPath);\n",
    "    new PrintWriter(outputStream)\n",
    "  }\n",
    "\n",
    "  def getFile(fileString: String): Array[String] ={\n",
    "    val inputPath = new Path(fileString)\n",
    "    val inputBuffer = scala.collection.mutable.ArrayBuffer.empty[String]\n",
    "    val iterator = inputPath.getFileSystem(new Configuration()).listFiles(inputPath, false)\n",
    "    while(iterator.hasNext()){\n",
    "        val fileStatus = iterator.next()\n",
    "        if(fileStatus.isFile()){\n",
    "          inputBuffer += fileStatus.getPath().toString()\n",
    "        }\n",
    "    }\n",
    "    inputBuffer.toArray\n",
    "  }\n",
    "\n",
    "\n",
    "val writer = null\n",
    "printSpark(writer, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Directed graph (each unordered pair of nodes is saved once): web-Google.txt \n",
      "# Webgraph from the Google programming contest, 2002\n",
      "# Nodes: 875713 Edges: 5105039\n",
      "# FromNodeId\tToNodeId\n",
      "0\t11342\n",
      "0\t824020\n",
      "0\t867923\n",
      "0\t891835\n",
      "11342\t0\n",
      "11342\t27469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = ./data/data.txt MapPartitionsRDD[1] at textFile at <console>:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "./data/data.txt MapPartitionsRDD[1] at textFile at <console>:35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.sparkContext.textFile(\"./data/data.txt\")\n",
    "data.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t11342\n",
      "0\t824020\n",
      "0\t867923\n",
      "0\t891835\n",
      "11342\t0\n",
      "Count: 5105039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rows = MapPartitionsRDD[2] at filter at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at filter at <console>:38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove Header\n",
    "val rows = data.filter(l => l.charAt(0) != '#')\n",
    "rows.take(5).foreach(println)\n",
    "println(\"Count: \"+rows.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729609,(3,4))\n",
      "(28995,(25,15))\n",
      "(892689,(4,4))\n",
      "(393372,(2,6))\n",
      "(581541,(2,4))\n",
      "(700977,(0,1))\n",
      "(781047,(14,14))\n",
      "(617493,(3,3))\n",
      "(432234,(2,2))\n",
      "(899514,(21,11))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flattenData = ShuffledRDD[4] at reduceByKey at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[4] at reduceByKey at <console>:50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flattenData = rows.\n",
    "    flatMap{ dataString =>\n",
    "        dataString.split(\"\\\\s+\").\n",
    "            zipWithIndex.\n",
    "            map{\n",
    "                case (value,index) => \n",
    "                    index match{\n",
    "                        case 0 => (value, (0, 1))\n",
    "                        case 1 => (value, (1, 0))\n",
    "                    }\n",
    "                }\n",
    "    }.reduceByKey{ case((in1, out1), (in2, out2)) => (in1+in2, out1+out2)}\n",
    "flattenData.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Find Out Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506742, 456\n",
      "305229, 372\n",
      "203748, 372\n",
      "768091, 330\n",
      "808643, 277\n",
      "412410, 268\n",
      "600479, 265\n",
      "376428, 258\n",
      "156950, 257\n",
      "885728, 256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "out = MapPartitionsRDD[10] at map at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at map at <console>:41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val out = flattenData.sortBy{case(id, (in, out)) => -out}.map{case(id, (in, out)) => id+\", \"+out}\n",
    "out.take(10).foreach(println)\n",
    "out.saveAsTextFile(\"./output/outDegree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Find In Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537039, 6326\n",
      "597621, 5354\n",
      "504140, 5271\n",
      "751384, 5182\n",
      "32163, 5097\n",
      "885605, 4847\n",
      "163075, 4731\n",
      "819223, 4620\n",
      "605856, 4550\n",
      "828963, 4484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "in = MapPartitionsRDD[17] at map at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[17] at map at <console>:41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val in = flattenData.sortBy{case(id, (in, out)) => -in}.map{case(id, (in, out)) => id+\", \"+in}\n",
    "in.take(10).foreach(println)\n",
    "in.saveAsTextFile(\"./output/inDegree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
