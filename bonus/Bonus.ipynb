{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "[Chicago Crime dataset](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2) - about 6 million reported records, 1.45G in size\n",
    "\n",
    "### Format\n",
    "One text file consisting of lines of records.\n",
    "\n",
    "1. ID\n",
    "+ Case number\n",
    "+ Date: date & time\n",
    "+ Block: text\n",
    "+ IUCR\n",
    "+ Primary type: text\n",
    "+ Description: text\n",
    "+ Location description: text\n",
    "+ Arrest: boolean\n",
    "+ Domestic\n",
    "+ Beat\n",
    "+ District\n",
    "+ Ward\n",
    "+ Community area\n",
    "+ FBI code\n",
    "+ X coordinate: numeric\n",
    "+ Y coordinate: numeric\n",
    "+ Year\n",
    "+ Updated on\n",
    "+ Latitude: numeric\n",
    "+ Longitude: numeric\n",
    "+ Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "3 subtasks:\n",
    "+ (30pt) For the attributes ‘Primary type’ and ‘Location description’, output the list of each value and the corresponding frequency count, sorted in descending order of the count, respectively.\n",
    "+ (30pt) Output the most frequently occurred ‘Primary type‘ for each possible value of ‘Location description’, sorted in descending order of the frequency count.\n",
    "+ (40pt) Output the most frequently occurred street name in the attribute ‘Block‘ for each ‘Primary type’, sorted in descending order of the frequency count. (You should remove the numbers in the ‘Block’ address of a street/avenue/boulevard)\n",
    "+ (Bonus) From the attribute ‘Date’, extract the time in hours and output the most frequently occurred hour for each ‘Primary type’ and ‘Location description’, sorted in descending order of the frequency count, respectively.\n",
    "\n",
    "### Output Format\n",
    "1. two sorted lists of (value, count)\n",
    "    + Sorted list of ‘Primary type’\n",
    "    + Sorted list of ‘Location description’\n",
    "1. n sorted lists of ‘Primary type’ for each ‘Location description’\n",
    "    + n: # of possible values for ‘Location description’\n",
    "1. n sorted list of street names for each ‘Primary type’\n",
    "    + n: # of possible values for ‘Primary type’\n",
    "1. two types of sorted lists:\n",
    "    + Sorted lists of hours for each possible ‘Primary type’\n",
    "    + Sorted lists of hours for each possible ‘Location description’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@17827ded\n",
      "Spark version:      2.3.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "printSample: (writer: Any, data: Any, title: String, format: String)Unit\n",
       "printSpark: (writer: Any, spark: org.apache.spark.sql.SparkSession)Unit\n",
       "outputWriter: (fileString: String)java.io.PrintWriter\n",
       "getFile: (fileString: String)Array[String]\n",
       "writer: Null = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.hadoop.fs._\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import java.io.{File,PrintWriter}\n",
    "\n",
    "  def printSample(writer: Any, data: Any, title: String = \"\", format: String = \"\"){\n",
    "    println(\"\\n\"+title+\" Data Sample: \" + format)\n",
    "    println(data+\"\\n\")\n",
    "  }\n",
    "\n",
    "  def printSpark(writer: Any, spark: SparkSession): Unit = {\n",
    "    println(\"Spark Entity:       \" + spark)\n",
    "    println(\"Spark version:      \" + spark.version)\n",
    "    println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "    println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)\n",
    "    println(\"\")\n",
    "  }\n",
    "\n",
    "  def outputWriter(fileString: String): PrintWriter ={\n",
    "    val outputPath = new Path(fileString)\n",
    "    val outputStream = outputPath.getFileSystem(new Configuration()).create(outputPath);\n",
    "    new PrintWriter(outputStream)\n",
    "  }\n",
    "\n",
    "  def getFile(fileString: String): Array[String] ={\n",
    "    val inputPath = new Path(fileString)\n",
    "    val inputBuffer = scala.collection.mutable.ArrayBuffer.empty[String]\n",
    "    val iterator = inputPath.getFileSystem(new Configuration()).listFiles(inputPath, false)\n",
    "    while(iterator.hasNext()){\n",
    "        val fileStatus = iterator.next()\n",
    "        if(fileStatus.isFile()){\n",
    "          inputBuffer += fileStatus.getPath().toString()\n",
    "        }\n",
    "    }\n",
    "    inputBuffer.toArray\n",
    "  }\n",
    "\n",
    "\n",
    "val writer = null\n",
    "printSpark(writer, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location\n",
      "10000092,HY189866,03/18/2015 07:44:00 PM,047XX W OHIO ST,041A,BATTERY,AGGRAVATED: HANDGUN,STREET,false,false,1111,011,28,25,04B,1144606,1903566,2015,02/10/2018 03:50:01 PM,41.891398861,-87.744384567,\"(41.891398861, -87.744384567)\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = ./data/data.csv MapPartitionsRDD[1] at textFile at <console>:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "./data/data.csv MapPartitionsRDD[1] at textFile at <console>:35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.sparkContext.textFile(\"./data/data.csv\")\n",
    "data.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000092,HY189866,03/18/2015 07:44:00 PM,047XX W OHIO ST,041A,BATTERY,AGGRAVATED: HANDGUN,STREET,false,false,1111,011,28,25,04B,1144606,1903566,2015,02/10/2018 03:50:01 PM,41.891398861,-87.744384567,\"(41.891398861, -87.744384567)\"\n",
      "\n",
      "Count: 6614026\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "header = ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location\n",
       "rows = MapPartitionsRDD[2] at filter at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at filter at <console>:39"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove Header\n",
    "val header = data.first()\n",
    "val rows = data.filter(l => l!=header)\n",
    "println(rows.first())\n",
    "println(\"\\nCount: \"+rows.count+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flatten Data Sample Data Sample: ( Primary Type, Location Description, Street, Date)\n",
      "\n",
      "(OTHER OFFENSE,VEHICLE NON-COMMERCIAL,LOREL AVE,01AM)\n",
      "(THEFT,RESIDENTIAL YARD (FRONT/BACK),WELLS ST,04PM)\n",
      "(BATTERY,APARTMENT,VINCENNES AVE,12AM)\n",
      "(DECEPTIVE PRACTICE,BANK,KARLOV AVE,07AM)\n",
      "(THEFT,RESIDENCE,LAWNDALE AVE,04PM)\n",
      "(OTHER OFFENSE,RESIDENCE,HENDERSON ST,05PM)\n",
      "(MOTOR VEHICLE THEFT,GAS STATION,ADDISON ST,04AM)\n",
      "(BURGLARY,ABANDONED BUILDING,103RD PL,08PM)\n",
      "(BURGLARY,APARTMENT,WASHTENAW AVE,11AM)\n",
      "(CRIM SEXUAL ASSAULT,APARTMENT,63RD ST,03PM)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "regex = \\d{2}\\/\\d{2}\\/\\d{4}\\s(\\d{2}):\\d{2}:\\d{2}\\s(\\wM)\n",
       "flattenData = MapPartitionsRDD[5] at map at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at map at <console>:46"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "val regex = \"\\\\d{2}\\\\/\\\\d{2}\\\\/\\\\d{4}\\\\s(\\\\d{2}):\\\\d{2}:\\\\d{2}\\\\s(\\\\wM)\".r\n",
    "val flattenData = spark.sparkContext.parallelize(rows.takeSample(false, 300, System.nanoTime.toInt)).\n",
    "    map{ dataString =>\n",
    "        val tuple = dataString.split(\",\").toSeq       \n",
    "        var list =  ListBuffer[String]()\n",
    "        for(m <- regex.findAllIn(tuple(2)).matchData;\n",
    "          e <- m.subgroups)\n",
    "          if(e!=null) list+=e\n",
    "        (tuple(5), tuple(7), tuple(3).split(\"^\\\\w+\\\\s*\\\\w*\\\\s*\")(1), list.toSeq.mkString(\"\"))\n",
    "    }\n",
    "printSample(writer, flattenData.take(10).mkString(\"\\n\"), \"Flatten Data Sample\", \"( Primary Type, Location Description, Street, Date)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Frequency Count of ‘Primary type’ and ‘Location description’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary Count Sample Data Sample: ( Primary Type, Count)\n",
      "(THEFT,68)\n",
      "(BATTERY,55)\n",
      "(NARCOTICS,37)\n",
      "(CRIMINAL DAMAGE,27)\n",
      "(BURGLARY,23)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "primeCount = MapPartitionsRDD[12] at sortBy at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at sortBy at <console>:47"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val primeCount = flattenData.map(tuple => (tuple._1, 1)).reduceByKey{(i, j) => i+j}.sortBy{case(prim, count) => -count}\n",
    "printSample(writer, primeCount.take(5).mkString(\"\\n\"), \"Primary Count Sample\", \"( Primary Type, Count)\")\n",
    "primeCount.saveAsTextFile(\"./output/task1/primeCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Location Count Sample Data Sample: ( Location Description, Count)\n",
      "\n",
      "(STREET,87)\n",
      "(RESIDENCE,48)\n",
      "(APARTMENT,30)\n",
      "(SIDEWALK,27)\n",
      "(OTHER,13)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "locCount = MapPartitionsRDD[20] at sortBy at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[20] at sortBy at <console>:49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locCount = flattenData.map(tuple => (tuple._2, 1)).reduceByKey{(i, j) => i+j}.sortBy{case(loc, count) => -count}\n",
    "printSample(writer, locCount.take(5).mkString(\"\\n\"), \"Location Count Sample\", \"( Location Description, Count)\\n\")\n",
    "primeCount.saveAsTextFile(\"./output/task1/locCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Primary Type Frequency Count per Location Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Location Group Sample Data Sample: ( Location Description, [(Primary Type, Count)])\n",
      "\n",
      "(ABANDONED BUILDING,CompactBuffer((BURGLARY,1)))\n",
      "(GOVERNMENT BUILDING/PROPERTY,CompactBuffer((BURGLARY,1)))\n",
      "(ALLEY,CompactBuffer((ROBBERY,2), (BATTERY,1), (CRIMINAL TRESPASS,1), (NARCOTICS,1), (CRIMINAL DAMAGE,1)))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "locGroup = ShuffledRDD[30] at groupByKey at <console>:51\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[30] at groupByKey at <console>:51"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locGroup = flattenData.map(tuple => ((tuple._2, tuple._1), 1)).\n",
    "                reduceByKey{(i, j) => i+j}.sortBy{case((loc, prime), count) => -count}.\n",
    "                map{\n",
    "                  tuple => (tuple._1._1, (tuple._1._2, tuple._2))  \n",
    "                }.groupByKey\n",
    "printSample(writer, locGroup.take(3).mkString(\"\\n\"), \"Location Group Sample\", \"( Location Description, [(Primary Type, Count)])\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locGroup.foreach{\n",
    "    case(loc, vect) =>\n",
    "    var newLoc = loc.split(\"\\\\W+\").mkString(\"_\")\n",
    "    val writer = outputWriter(\"./output/task2/\"+newLoc+\".csv\")\n",
    "    vect.foreach{\n",
    "        vectString => writer.println(vectString)\n",
    "    }\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Street Name Frequency Count per Primary Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary Group Sample Data Sample: ( Primary Type, [(Street, Count)])\n",
      "\n",
      "(CRIM SEXUAL ASSAULT,CompactBuffer((63RD ST,1)))\n",
      "(WEAPONS VIOLATION,CompactBuffer((EAST END AVE,1), (MUSKEGON AVE,1), (SOUTH CHICAGO AVE,1), (RIDGELAND AVE,1), (FLOURNOY ST,1)))\n",
      "(PROSTITUTION,CompactBuffer((CICERO AVE,1), (CHICAGO AVE,1), (KOSTNER AVE,1), (KEATING AVE,1)))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "primeGroup = ShuffledRDD[39] at groupByKey at <console>:54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[39] at groupByKey at <console>:54"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val primeGroup = flattenData.map{\n",
    "                    tuple => \n",
    "                    ( ( tuple._1, tuple._3 ), 1)\n",
    "                }.\n",
    "                reduceByKey{(i, j) => i+j}.sortBy{case((prime, loc), count) => -count}.\n",
    "                map{\n",
    "                  tuple => (tuple._1._1, (tuple._1._2, tuple._2))  \n",
    "                }.groupByKey\n",
    "printSample(writer, primeGroup.take(3).mkString(\"\\n\"), \"Primary Group Sample\", \"( Primary Type, [(Street, Count)])\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeGroup.foreach{\n",
    "    case(prime, vect) =>\n",
    "    var newPrime = prime.split(\"\\\\W+\").mkString(\"_\")\n",
    "    val writer = outputWriter(\"./output/task3/\"+newPrime+\".csv\")\n",
    "    vect.foreach{\n",
    "        vectString => writer.println(vectString)\n",
    "    }\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus - Hour Frequency Count per Primary Type and Location Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary Hours Sample Data Sample: ( Primary Type, [(Hours, Count)])\n",
      "\n",
      "(ROBBERY,CompactBuffer((11PM,2), (01AM,1), (10AM,1), (03AM,1), (12AM,1), (06PM,1)))\n",
      "(CONCEALED CARRY LICENSE VIOLATION,CompactBuffer((10PM,1)))\n",
      "(GAMBLING,CompactBuffer((08PM,1)))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "primeHours = ShuffledRDD[48] at groupByKey at <console>:55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[48] at groupByKey at <console>:55"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val primeHours = flattenData.map{\n",
    "                    tuple => \n",
    "                    ( ( tuple._1,  tuple._4), 1)\n",
    "                }.\n",
    "                reduceByKey{(i, j) => i+j}.sortBy{case((prime, hours), count) => -count}.\n",
    "                map{\n",
    "                  tuple => (tuple._1._1, (tuple._1._2, tuple._2))  \n",
    "                }.groupByKey\n",
    "printSample(writer, primeHours.take(3).mkString(\"\\n\"), \"Primary Hours Sample\", \"( Primary Type, [(Hours, Count)])\\n\")\n",
    "\n",
    "primeHours.foreach{\n",
    "    case(prime, vect) =>\n",
    "    var newPrime = prime.split(\"\\\\W+\").mkString(\"_\")\n",
    "    val writer = outputWriter(\"./output/task4-prime/\"+newPrime+\".csv\")\n",
    "    vect.foreach{\n",
    "        vectString => writer.println(vectString)\n",
    "    }\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Location Hours Sample Data Sample: ( Location Type, [(Hours, Count)])\n",
      "\n",
      "(ABANDONED BUILDING,CompactBuffer((08PM,1)))\n",
      "(GOVERNMENT BUILDING/PROPERTY,CompactBuffer((05PM,1)))\n",
      "(ALLEY,CompactBuffer((11PM,2), (01AM,1), (09PM,1), (07PM,1), (10AM,1)))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "locHours = ShuffledRDD[57] at groupByKey at <console>:55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[57] at groupByKey at <console>:55"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val locHours = flattenData.map{\n",
    "                    tuple => \n",
    "                    ( ( tuple._2,  tuple._4), 1)\n",
    "                }.\n",
    "                reduceByKey{(i, j) => i+j}.sortBy{case((loc, hours), count) => -count}.\n",
    "                map{\n",
    "                  tuple => (tuple._1._1, (tuple._1._2, tuple._2))  \n",
    "                }.groupByKey\n",
    "printSample(writer, locHours.take(3).mkString(\"\\n\"), \"Location Hours Sample\", \"( Location Type, [(Hours, Count)])\\n\")\n",
    "\n",
    "locHours.foreach{\n",
    "    case(loc, vect) =>\n",
    "    var newLoc = loc.split(\"\\\\W+\").mkString(\"_\")\n",
    "    val writer = outputWriter(\"./output/task4-loc/\"+newLoc+\".csv\")\n",
    "    vect.foreach{\n",
    "        vectString => writer.println(vectString)\n",
    "    }\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
