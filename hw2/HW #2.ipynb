{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "[News Popularity in Multiple Social Media Platforms Data Set](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms) - 13 CSV files, 155MB in total  \n",
    "\n",
    "This dataset contains a large set of news items and their respective social feedback on Facebook, Google + and LinkedIn.\n",
    "\n",
    "\n",
    "### Format\n",
    "One CSV File with News Data Records and 12 CSV Files with Social Feedback.\n",
    "The Social Feedback File contains the feedback from one of the social platforms {Facebook, Google+, LinkedIn} on one of the topics {Economy, Microsoft, Palestine, Obama}.\n",
    "\n",
    "#### News Data Variables\n",
    "Each record contains 11 attributes\n",
    "\n",
    "1. IDLink (numeric): Unique identifier of news items \n",
    "2. Title (string): Title of the news item according to the official media sources \n",
    "3. Headline (string): Headline of the news item according to the official media sources \n",
    "4. Source (string): Original news outlet that published the news item \n",
    "5. Topic (string): Query topic used to obtain the items in the official media sources \n",
    "6. PublishDate (timestamp): Date and time of the news items' publication \n",
    "7. SentimentTitle (numeric): Sentiment score of the text in the news items' title \n",
    "8. SentimentHeadline (numeric): Sentiment score of the text in the news items' headline \n",
    "9. Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook \n",
    "10. GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+ \n",
    "11. LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn \n",
    "\n",
    "#### Social Feedback Variables\n",
    "Each record contains 145 attributes\n",
    "\n",
    "1. IDLink (numeric): Unique identifier of news items \n",
    "2. TS1 (numeric): Level of popularity in time slice 1 (0-20 minutes upon publication) \n",
    "3. TS2 (numeric): Level of popularity in time slice 2 (20-40 minutes upon publication) \n",
    "4. TS... (numeric): Level of popularity in time slice ... \n",
    "5. TS144 (numeric): Final level of popularity after 2 days upon publication\n",
    "\n",
    "\n",
    "### Task\n",
    "4 subtasks:\n",
    "+ (20pt) In social feedback data, calculate the average popularity of each news by hour, and by day, respectively\n",
    "+ (20pt) In news data, calculate the sum and average sentiment score of each topic, respectively\n",
    "+ (30pt) In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively\n",
    "+ (30pt) From the previous subtask, for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@7dc75eac\n",
      "Spark version:      2.3.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n"
     ]
    }
   ],
   "source": [
    "// Pre-Configured Spark Context in sc\n",
    "\n",
    "println(\"Spark Entity:       \" + spark)\n",
    "println(\"Spark version:      \" + spark.version)\n",
    "println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Average Popularity (By Hour, By Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Files: \n",
      "     ./data/social/Facebook_Economy.csv\n",
      "\n",
      "Loaded Data Sample: ((1,0),(-1,1.0)) -- ((UID, Hour), (Popularity, Count))"
     ]
    }
   ],
   "source": [
    "val folder = \"./data/social/\"\n",
    "//val topics = Seq(\"Economy\", \"Microsoft\", \"Obama\", \"Palestine\")\n",
    "val topics = Seq(\"Economy\")\n",
    "//val platforms = Seq(\"Facebook\", \"GooglePlus\", \"LinkedIn\")\n",
    "val platforms = Seq(\"Facebook\")\n",
    "\n",
    "var topic = \"\"\n",
    "var platform = \"\"\n",
    "var flattenSocialData = spark.sparkContext.emptyRDD[((String, Int), (Int, Double))]\n",
    "\n",
    "println(\"Loaded Files: \")\n",
    "for(topic <- topics; platform <- platforms){\n",
    "    val loc = folder+platform+\"_\"+topic+\".csv\"\n",
    "    val testData = spark.sparkContext.textFile(loc)\n",
    "    val header = testData.first\n",
    "    val flattenData = testData.filter(l => l != header).\n",
    "    flatMap{ dataString =>\n",
    "        val attr = dataString.split(\",\")\n",
    "        attr.zipWithIndex.\n",
    "        filter{\n",
    "            case (value,index) => index >= 1\n",
    "        }.map{\n",
    "            case (value,index) => ((attr(0),(index-1)/3),(value.toInt,1.0))\n",
    "        }\n",
    "    }\n",
    "    flattenSocialData = flattenSocialData.union(flattenData)\n",
    "    println(\"     \"+loc)\n",
    "}\n",
    "\n",
    "print(\"\\nLoaded Data Sample: \")\n",
    "flattenSocialData.take(1).foreach(print)\n",
    "print(\" -- ((UID, Hour), (Popularity, Count))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10974,1),(-3,3.0))((48028,27),(267,3.0))((9730,14),(0,3.0))((14772,23),(170,3.0))((11423,7),(6,3.0))"
     ]
    }
   ],
   "source": [
    "flattenSocialData.persist()\n",
    "\n",
    "val pop_by_hour = flattenSocialData.\n",
    "    reduceByKey{case ((ia, ib), (ja, jb)) => (ia+ja, ib+jb)}\n",
    "\n",
    "pop_by_hour.take(5).foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21508,(d,0,2.486111111111111))(783,(d,0,47.02777777777778))(10974,(d,1,24.166666666666668))(46034,(d,0,4.0))(51147,(d,1,23.055555555555557))"
     ]
    }
   ],
   "source": [
    "val pop_by_day = flattenSocialData.\n",
    "    map{case((uid, hr), (sum, count)) => ((uid, hr/24), (sum/count, 1))}.\n",
    "    reduceByKey{case ((ia, ib), (ja, jb)) => (ia+ja, ib+jb)}.\n",
    "    map{case((uid, day), (sum, count)) => (uid,(\"d\", day, sum / count))}\n",
    "    \n",
    "pop_by_day.take(5).foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,CompactBuffer((h,32,12.0), (h,3,7.0), (h,6,8.0), (h,37,12.0), (h,25,12.0), (h,9,8.0), (h,0,-1.0), (h,4,7.666666666666667), (h,34,12.0), (h,29,12.0), (h,13,9.333333333333334), (h,26,12.0), (h,44,13.0), (h,41,12.0), (h,30,12.0), (h,5,8.0), (h,10,8.0), (h,15,10.666666666666666), (h,16,11.0), (h,23,12.0), (h,42,13.0), (h,8,8.0), (h,33,12.0), (h,20,12.0), (h,45,13.0), (h,2,1.6666666666666667), (h,22,12.0), (h,17,11.333333333333334), (h,38,12.0), (h,36,12.0), (h,18,12.0), (h,43,13.0), (h,19,12.0), (h,7,8.0), (h,11,9.0), (h,24,12.0), (h,1,-1.0), (h,35,12.0), (h,39,12.0), (h,47,13.0), (h,21,12.0), (h,12,9.0), (h,28,12.0), (h,40,12.0), (h,27,12.0), (h,31,12.0), (h,46,13.0), (h,14,10.0), (d,0,8.527777777777779), (d,1,12.25)))"
     ]
    }
   ],
   "source": [
    "val all_pop = pop_by_hour.\n",
    "    map{case((uid, hr), (sum, count)) => (uid,(\"h\", hr, sum / count))}.\n",
    "    union(pop_by_day).\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true)\n",
    "    \n",
    "all_pop.take(1).foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,8.527777777777779,12.25,-1.0,-1.0,1.6666666666666667,7.0,7.666666666666667,8.0,8.0,8.0,8.0,8.0,8.0,9.0,9.0,9.333333333333334,10.0,10.666666666666666,11.0,11.333333333333334,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,12.0,13.0,13.0,13.0,13.0,13.0,13.0\n"
     ]
    }
   ],
   "source": [
    "val all_tuple = all_pop.map { case(uid, iterable) => \n",
    "    val vect = iterable.toVector.sortBy { tup => \n",
    "        (tup._1, tup._2)\n",
    "    }.map{ case (doh, no, value) => value }\n",
    "    uid+\",\"+vect.mkString(\",\")\n",
    "}\n",
    "\n",
    "all_tuple.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tuple.saveAsTextFile(\"./data/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
