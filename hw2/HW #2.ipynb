{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "[News Popularity in Multiple Social Media Platforms Data Set](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms) - 13 CSV files, 155MB in total  \n",
    "\n",
    "This dataset contains a large set of news items and their respective social feedback on Facebook, Google + and LinkedIn.\n",
    "\n",
    "\n",
    "### Format\n",
    "One CSV File with News Data Records and 12 CSV Files with Social Feedback.\n",
    "The Social Feedback File contains the feedback from one of the social platforms {Facebook, Google+, LinkedIn} on one of the topics {Economy, Microsoft, Palestine, Obama}.\n",
    "\n",
    "#### News Data Variables\n",
    "Each record contains 11 attributes\n",
    "\n",
    "1. IDLink (numeric): Unique identifier of news items \n",
    "2. Title (string): Title of the news item according to the official media sources \n",
    "3. Headline (string): Headline of the news item according to the official media sources \n",
    "4. Source (string): Original news outlet that published the news item \n",
    "5. Topic (string): Query topic used to obtain the items in the official media sources \n",
    "6. PublishDate (timestamp): Date and time of the news items' publication \n",
    "7. SentimentTitle (numeric): Sentiment score of the text in the news items' title \n",
    "8. SentimentHeadline (numeric): Sentiment score of the text in the news items' headline \n",
    "9. Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook \n",
    "10. GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+ \n",
    "11. LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn \n",
    "\n",
    "#### Social Feedback Variables\n",
    "Each record contains 145 attributes\n",
    "\n",
    "1. IDLink (numeric): Unique identifier of news items \n",
    "2. TS1 (numeric): Level of popularity in time slice 1 (0-20 minutes upon publication) \n",
    "3. TS2 (numeric): Level of popularity in time slice 2 (20-40 minutes upon publication) \n",
    "4. TS... (numeric): Level of popularity in time slice ... \n",
    "5. TS144 (numeric): Final level of popularity after 2 days upon publication\n",
    "\n",
    "\n",
    "### Task\n",
    "4 subtasks:\n",
    "+ (20pt) In social feedback data, calculate the average popularity of each news by hour, and by day, respectively\n",
    "+ (20pt) In news data, calculate the sum and average sentiment score of each topic, respectively\n",
    "+ (30pt) In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively\n",
    "+ (30pt) From the previous subtask, for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively\n",
    "\n",
    "### Implementation Issues\n",
    "+ Large number of Attributes for each record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@782d2260\n",
      "Spark version:      2.2.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n"
     ]
    }
   ],
   "source": [
    "// Pre-Configured Spark Context in sc\n",
    "\n",
    "println(\"Spark Entity:       \" + spark)\n",
    "println(\"Spark version:      \" + spark.version)\n",
    "println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Average Popularity (By Hour, By Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputBuffer = ArrayBuffer()\n",
       "inputPath = data/social/test\n",
       "iterator = org.apache.hadoop.fs.FileSystem$6@3578b767\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.hadoop.fs.FileSystem$6@3578b767"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.fs._\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "val inputBuffer = scala.collection.mutable.ArrayBuffer.empty[String]\n",
    "\n",
    "val inputPath = new Path(\"./data/social/test\")\n",
    "val iterator = inputPath.getFileSystem(new Configuration()).listFiles(inputPath, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/home/micky/big_data/hw2/data/social/test/Facebook_Palestine.csv\n",
      "file:/home/micky/big_data/hw2/data/social/test/GooglePlus_Palestine.csv\n",
      "file:/home/micky/big_data/hw2/data/social/test/GooglePlus_Microsoft.csv\n"
     ]
    }
   ],
   "source": [
    "while(iterator.hasNext()){\n",
    "    val fileStatus = iterator.next()\n",
    "    inputBuffer += fileStatus.getPath().toString()\n",
    "}\n",
    "inputBuffer.toArray.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Data Sample: ((61974,0),(-1.0,1)) -- ((UID, Hour), (Popularity, Count))"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flattenSocialData = UnionRDD[15] at union at <console>:48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[15] at union at <console>:48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var flattenSocialData = spark.sparkContext.emptyRDD[((String, Int), (Double, Int))]\n",
    "\n",
    "inputBuffer.toArray.foreach{ input =>\n",
    "    val data = spark.sparkContext.textFile(input)\n",
    "    val header = data.first\n",
    "    val flattenData = data.filter(l => l != header).\n",
    "    flatMap{ dataString =>\n",
    "        val attr = dataString.split(\",\")\n",
    "        attr.zipWithIndex.\n",
    "        filter{\n",
    "            case (value,index) => index >= 1\n",
    "        }.map{\n",
    "            case (value,index) => ((attr(0),(index-1)/3),(value.toDouble,1))\n",
    "        }\n",
    "    }\n",
    "    flattenSocialData = flattenSocialData.union(flattenData)\n",
    "}\n",
    "\n",
    "print(\"\\nLoaded Data Sample: \")\n",
    "flattenSocialData.take(1).foreach(print)\n",
    "print(\" -- ((UID, Hour), (Popularity, Count))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "((16768,38),(10.0,3)) -- ((UID, Hour), (Popularity, Count))                     "
     ]
    },
    {
     "data": {
      "text/plain": [
       "pop_by_hour = ShuffledRDD[16] at reduceByKey at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[16] at reduceByKey at <console>:36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenSocialData.persist()\n",
    "\n",
    "val pop_by_hour = flattenSocialData.\n",
    "    reduceByKey{case ((ia, ib), (ja, jb)) => (ia+ja, ib+jb)}\n",
    "\n",
    "print(\"\\nIntermediate Data Sample: \")\n",
    "pop_by_hour.take(1).foreach(print)\n",
    "print(\" -- ((UID, Hour), (Popularity, Count))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(18258,(Day,1,1.0)) -- ((UID, ('Day', No., Popularity Average))                 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "pop_by_day = MapPartitionsRDD[19] at map at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[19] at map at <console>:36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pop_by_day = flattenSocialData.\n",
    "    map{case((uid, hr), (sum, count)) => ((uid, hr/24), (sum/count, 1))}.\n",
    "    reduceByKey{case ((ia, ib), (ja, jb)) => (ia+ja, ib+jb)}.\n",
    "    map{case((uid, day), (sum, count)) => (uid,(\"Day\", day, sum / count))};\n",
    "    \n",
    "\n",
    "print(\"\\nPopularity by Day Data Sample: \")\n",
    "pop_by_day.take(1).foreach(print)\n",
    "print(\" -- ((UID, ('Day', No., Popularity Average))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11:================================================>         (5 + 1) / 6]\n",
      "[Stage 15:================================================>         (5 + 1) / 6](99997,CompactBuffer((Hour,0,0.0), (Hour,15,27.833333333333332), (Hour,18,38.5), (Hour,21,58.666666666666664), (Hour,14,26.166666666666668), (Hour,22,64.66666666666667), (Hour,32,110.83333333333333), (Hour,9,12.333333333333334), (Hour,31,108.83333333333333), (Hour,34,111.0), (Hour,13,24.0), (Hour,5,4.5), (Hour,26,80.83333333333333), (Hour,4,3.8333333333333335), (Hour,25,77.33333333333333), (Hour,47,148.0), (Hour,24,74.5), (Hour,41,140.33333333333334), (Hour,20,52.666666666666664), (Hour,10,16.666666666666668), (Hour,17,32.166666666666664), (Hour,37,130.16666666666666), (Hour,29,100.66666666666667), (Hour,6,5.5), (Hour,46,146.66666666666666), (Hour,38,132.16666666666666), (Hour,3,2.6666666666666665), (Hour,36,111.0), (Hour,30,104.5), (Hour,8,6.666666666666667), (Hour,16,28.0), (Hour,35,111.0), (Hour,11,20.0), (Hour,2,1.0), (Hour,42,143.0), (Hour,19,44.666666666666664), (Hour,43,144.66666666666666), (Hour,40,138.5), (Hour,39,134.5), (Hour,44,145.0), (Hour,45,145.5), (Hour,23,70.33333333333333), (Hour,33,111.0), (Hour,28,95.83333333333333), (Hour,7,5.5), (Hour,12,21.333333333333332), (Hour,1,0.0), (Hour,27,88.16666666666667), (Day,0,23.65277777777778), (Day,1,118.08333333333333)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "all_pop = ShuffledRDD[25] at sortByKey at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[25] at sortByKey at <console>:41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val all_pop = pop_by_hour.\n",
    "    map{case((uid, hr), (sum, count)) => (uid,(\"Hour\", hr, sum / count))}.\n",
    "    union(pop_by_day).\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = false)\n",
    "    \n",
    "print(\"\\nAll Popularity Data Sample: \")\n",
    "all_pop.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted Data Values for 99997: Vector((Day,0,23.65277777777778), (Day,1,118.08333333333333), (Hour,0,0.0), (Hour,1,0.0), (Hour,2,1.0), (Hour,3,2.6666666666666665), (Hour,4,3.8333333333333335), (Hour,5,4.5), (Hour,6,5.5), (Hour,7,5.5), (Hour,8,6.666666666666667), (Hour,9,12.333333333333334), (Hour,10,16.666666666666668), (Hour,11,20.0), (Hour,12,21.333333333333332), (Hour,13,24.0), (Hour,14,26.166666666666668), (Hour,15,27.833333333333332), (Hour,16,28.0), (Hour,17,32.166666666666664), (Hour,18,38.5), (Hour,19,44.666666666666664), (Hour,20,52.666666666666664), (Hour,21,58.666666666666664), (Hour,22,64.66666666666667), (Hour,23,70.33333333333333), (Hour,24,74.5), (Hour,25,77.33333333333333), (Hour,26,80.83333333333333), (Hour,27,88.16666666666667), (Hour,28,95.83333333333333), (Hour,29,100.66666666666667), (Hour,30,104.5), (Hour,31,108.83333333333333), (Hour,32,110.83333333333333), (Hour,33,111.0), (Hour,34,111.0), (Hour,35,111.0), (Hour,36,111.0), (Hour,37,130.16666666666666), (Hour,38,132.16666666666666), (Hour,39,134.5), (Hour,40,138.5), (Hour,41,140.33333333333334), (Hour,42,143.0), (Hour,43,144.66666666666666), (Hour,44,145.0), (Hour,45,145.5), (Hour,46,146.66666666666666), (Hour,47,148.0))"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_tuple = (99997,CompactBuffer((Hour,0,0.0), (Hour,15,27.833333333333332), (Hour,18,38.5), (Hour,21,58.666666666666664), (Hour,14,26.166666666666668), (Hour,22,64.66666666666667), (Hour,32,110.83333333333333), (Hour,9,12.333333333333334), (Hour,31,108.83333333333333), (Hour,34,111.0), (Hour,13,24.0), (Hour,5,4.5), (Hour,26,80.83333333333333), (Hour,4,3.8333333333333335), (Hour,25,77.33333333333333), (Hour,47,148.0), (Hour,24,74.5), (Hour,41,140.33333333333334), (Hour,20,52.666666666666664), (Hour,10,16.666666666666668), (Hour,17,32.166666666666664), (Hour,37,130.16666666666666), (Hour,29,100.66666666666667), (Hour,6,5.5), (Hour,46,146.66666666666666), (Hour,38,132.16666666666666), (Hour,3,2.6666666666666665), (Hour,36,111.0), (Hour,30,104.5)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(99997,CompactBuffer((Hour,0,0.0), (Hour,15,27.833333333333332), (Hour,18,38.5), (Hour,21,58.666666666666664), (Hour,14,26.166666666666668), (Hour,22,64.66666666666667), (Hour,32,110.83333333333333), (Hour,9,12.333333333333334), (Hour,31,108.83333333333333), (Hour,34,111.0), (Hour,13,24.0), (Hour,5,4.5), (Hour,26,80.83333333333333), (Hour,4,3.8333333333333335), (Hour,25,77.33333333333333), (Hour,47,148.0), (Hour,24,74.5), (Hour,41,140.33333333333334), (Hour,20,52.666666666666664), (Hour,10,16.666666666666668), (Hour,17,32.166666666666664), (Hour,37,130.16666666666666), (Hour,29,100.66666666666667), (Hour,6,5.5), (Hour,46,146.66666666666666), (Hour,38,132.16666666666666), (Hour,3,2.6666666666666665), (Hour,36,111.0), (Hour,30,104.5), (Hour,8,6.666666666666667), (Hour,16,28.0), (Hour,35,111.0), (Hour,11,20.0), (Hour,2,1.0), (Hour,42,143.0), (Hour,19,44.666666666666664), (Hour,43,144.66666666666666), (Hour,40,138.5), (Hour,39,134.5), (Hour,44,145.0), (Hour,45,145.5), (Hour,23,70.33333333333333), (Hour,33,111.0), (Hour,28,95.83333333333333), (Hour,7,5.5), (Hour,12,21.333333333333332), (Hour,1,0.0), (Hour,27,88.16666666666667), (Day,0,23.65277777777778), (Day,1,118.08333333333333)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_tuple = all_pop.first()\n",
    "print(\"\\nSorted Data Values for \"+test_tuple._1+\": \")\n",
    "print(test_tuple._2.toVector.sortBy { tup => (tup._1, tup._2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finalized Data Sample: 99997,23.65277777777778,118.08333333333333,0.0,0.0,1.0,2.6666666666666665,3.8333333333333335,4.5,5.5,5.5,6.666666666666667,12.333333333333334,16.666666666666668,20.0,21.333333333333332,24.0,26.166666666666668,27.833333333333332,28.0,32.166666666666664,38.5,44.666666666666664,52.666666666666664,58.666666666666664,64.66666666666667,70.33333333333333,74.5,77.33333333333333,80.83333333333333,88.16666666666667,95.83333333333333,100.66666666666667,104.5,108.83333333333333,110.83333333333333,111.0,111.0,111.0,111.0,130.16666666666666,132.16666666666666,134.5,138.5,140.33333333333334,143.0,144.66666666666666,145.0,145.5,146.66666666666666,148.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "all_tuple = MapPartitionsRDD[26] at map at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at map at <console>:39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val all_tuple = all_pop.map { case(uid, iterable) => \n",
    "    val vect = iterable.toVector.sortBy { tup => \n",
    "        (tup._1, tup._2)\n",
    "    }.map{ case (doh, no, value) => value }\n",
    "    uid+\",\"+vect.mkString(\",\")\n",
    "}\n",
    "print(\"\\nFinalized Data Sample: \")\n",
    "all_tuple.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 31:================================================>         (5 + 1) / 6]"
     ]
    }
   ],
   "source": [
    "all_tuple.saveAsTextFile(\"./output_popularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Sum and Average Sentiment Score For Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsData_string = ./data/news.csv MapPartitionsRDD[29] at textFile at <console>:31\n",
       "header = \"IDLink\",\"Title\",\"Headline\",\"Source\",\"Topic\",\"PublishDate\",\"SentimentTitle\",\"SentimentHeadline\",\"Facebook\",\"GooglePlus\",\"LinkedIn\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"IDLink\",\"Title\",\"Headline\",\"Source\",\"Topic\",\"PublishDate\",\"SentimentTitle\",\"SentimentHeadline\",\"Facebook\",\"GooglePlus\",\"LinkedIn\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newsData_string = sc.textFile(\"./data/news.csv\")\n",
    "val header = newsData_string.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99248\n",
      "Obama Lays Wreath at Arlington National Cemetery\n",
      "Obama Lays Wreath at Arlington National Cemetery. President Barack Obama has laid a wreath at the Tomb of the Unknowns to honor\n",
      "USA TODAY\n",
      "obama\n",
      "2002-04-02 00:00:00\n",
      "0\n",
      "-0.0533001790889026\n",
      "-1\n",
      "-1\n",
      "-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "regex = ,([\\d.-]+)$|^([\\d.-]+)(?=,)|(?:,)([\\d.-]+)(?=,)|(?:,\"+)((?:[^\"]+\"{3,})*[^\"]*)(?=\"+,)\n",
       "newsParse = MapPartitionsRDD[31] at map at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[31] at map at <console>:36"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "var regex = \"\"\",([\\d.-]+)$|^([\\d.-]+)(?=,)|(?:,)([\\d.-]+)(?=,)|(?:,\"+)((?:[^\"]+\"{3,})*[^\"]*)(?=\"+,)\"\"\".r\n",
    "var newsParse = newsData_string.filter(x => x!=header).map{\n",
    "    string =>\n",
    "    var list =  ListBuffer[String]()\n",
    "    for(m <- regex.findAllIn(string).matchData;\n",
    "      e <- m.subgroups)\n",
    "      if(e!=null) list+=e\n",
    "    list.toSeq\n",
    "}\n",
    "newsParse.first().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flatten Sentient Score Sample: (obama,(0.0,-0.0533001790889026,1)) -- (Topic, (Title Sentient Score, Headline Sentient Score, Count))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flattenSentientScore = MapPartitionsRDD[32] at map at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[32] at map at <console>:39"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flattenSentientScore = newsParse.\n",
    "  map{ attr =>\n",
    "      (attr(4),(attr(6).toString.toDouble, attr(7).toString.toDouble, 1))\n",
    "  }\n",
    "\n",
    "print(\"\\nFlatten Sentient Score Sample: \")\n",
    "flattenSentientScore.take(1).foreach(print)\n",
    "print(\" -- (Topic, (Title Sentient Score, Headline Sentient Score, Count))\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced Sentient Score Sample: (economy,(0.09550339823478982,-0.3221036949470573,9)) -- (Topic, (Title Sentient Score, Headline Sentient Score, Count))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "smallerSampleSize = ParallelCollectionRDD[34] at parallelize at <console>:40\n",
       "reducedSentientScore = ShuffledRDD[35] at reduceByKey at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[35] at reduceByKey at <console>:41"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var smallerSampleSize = sc.parallelize(flattenSentientScore.takeSample(false, 30, System.nanoTime.toInt))\n",
    "var reducedSentientScore = smallerSampleSize.reduceByKey(\n",
    "    (a,b) =>\n",
    "    (a._1 + b._1, a._2+ b._2, a._3 + b._3)\n",
    ")\n",
    "\n",
    "print(\"\\nReduced Sentient Score Sample: \")\n",
    "reducedSentientScore.take(1).foreach(print)\n",
    "print(\" -- (Topic, (Title Sentient Score, Headline Sentient Score, Count))\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Sentient Score: \n",
      "\n",
      "For economy (9 entries): \n",
      "        Sum of Title Sentient Score: 0.09550339823478982\n",
      "        Sum of Headline Sentient Score: -0.3221036949470573\n",
      "        Average of Title Sentient Score: 0.010611488692754424\n",
      "        Average of Headline Sentient Score: -0.035789299438561926\n",
      "\n",
      "For obama (10 entries): \n",
      "        Sum of Title Sentient Score: -0.3022366713719931\n",
      "        Sum of Headline Sentient Score: -0.4927365148886257\n",
      "        Average of Title Sentient Score: -0.03022366713719931\n",
      "        Average of Headline Sentient Score: -0.04927365148886257\n",
      "\n",
      "For microsoft (9 entries): \n",
      "        Sum of Title Sentient Score: -0.2767157306946107\n",
      "        Sum of Headline Sentient Score: -0.43691497050841\n",
      "        Average of Title Sentient Score: -0.03074619229940119\n",
      "        Average of Headline Sentient Score: -0.04854610783426778\n",
      "\n",
      "For palestine (2 entries): \n",
      "        Sum of Title Sentient Score: -0.0947711880323037\n",
      "        Sum of Headline Sentient Score: 0.2604403359164371\n",
      "        Average of Title Sentient Score: -0.04738559401615185\n",
      "        Average of Headline Sentient Score: 0.13022016795821856\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finalSentientScore = MapPartitionsRDD[36] at map at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "myprint: (s: (Any, Double, Double, Double, Double, Any))Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[36] at map at <console>:43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var finalSentientScore = reducedSentientScore.map{\n",
    "    case (topic,(titleSum, headSum, count)) =>\n",
    "    (topic, titleSum, headSum, titleSum/count, headSum/count, count)\n",
    "}\n",
    "\n",
    "def myprint(s: Tuple6[Any, Double, Double, Double, Double, Any]): Unit = {\n",
    "        println(\"For \"+s._1+\" (\"+s._6+\" entries): \")\n",
    "        println(\"        Sum of Title Sentient Score: \" + s._2)\n",
    "        println(\"        Sum of Headline Sentient Score: \" + s._3)\n",
    "        println(\"        Average of Title Sentient Score: \" + s._4)\n",
    "        println(\"        Average of Headline Sentient Score: \" + s._5 + \"\\n\")\n",
    "    }\n",
    "\n",
    "println(\"\\nFinal Sentient Score: \\n\")\n",
    "finalSentientScore.collect().foreach(myprint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Title/Headline Word Count in Descending Order (In Total, Per Day, and Per Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsData_string = ./data/news.csv MapPartitionsRDD[12] at textFile at <console>:35\n",
       "header = \"IDLink\",\"Title\",\"Headline\",\"Source\",\"Topic\",\"PublishDate\",\"SentimentTitle\",\"SentimentHeadline\",\"Facebook\",\"GooglePlus\",\"LinkedIn\"\n",
       "regex = ,([\\d.-]+)$|^([\\de+.-]+)(?=,)|(?:,)([\\d.-]+)(?=,)|(?:,\"+)((?:[^\"]+\"{3,})*[^\"]*)(?=\"+,)\n",
       "newsParse = MapPartitionsRDD[14] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(99248, Obama Lays Wreath at Arlington National Cemetery, Obama Lays Wreath at Arlington National Cemetery. President Barack Obama has laid a wreath at the Tomb of the Unknowns to honor, USA TODAY, obama, 2002-04-02 00:00:00, 0, -0.0533001790889026, -1, -1, -1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val newsData_string = sc.textFile(\"./data/news.csv\")\n",
    "val header = newsData_string.first()\n",
    "var regex = \"\"\",([\\d.-]+)$|^([\\de+.-]+)(?=,)|(?:,)([\\d.-]+)(?=,)|(?:,\"+)((?:[^\"]+\"{3,})*[^\"]*)(?=\"+,)\"\"\".r\n",
    "var newsParse = newsData_string.filter(x => x!=header).map{\n",
    "    string =>\n",
    "    var list =  ListBuffer[String]()\n",
    "    for(m <- regex.findAllIn(string).matchData;\n",
    "      e <- m.subgroups)\n",
    "      if(e!=null) list+=e\n",
    "    list.toSeq\n",
    "}\n",
    "newsParse.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPreProcess: (input: Any)Array[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[this, is, a, test, string, yrs, i'm, hoping, that, this, would, work, 103, yrs, $3,000, this, is, a, book]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordPreProcess(input: Any): Array[String] = {\n",
    "        var matchRegex = \"\"\"([$]?(?:[\\w]+(?:[\\w',]*[\\w]+)+|[\\w]))\"\"\".r\n",
    "        var list =  ListBuffer[String]()\n",
    "        for(m <- matchRegex.findAllIn(input.toString.toLowerCase).matchData;\n",
    "          e <- m.subgroups)\n",
    "          list+=e\n",
    "        list.toArray\n",
    "    }\n",
    "\n",
    "wordPreProcess(\"This is a test string, yrs' I'm hoping that this would work!! :) 103 yrs $3,000 \\n'This is a book;'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flatten Word Sample: (watch,title,palestine,2015-10-21) -- (Word, 'Title'/'Headline', Topic, Date)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "smallerSampleSize = ParallelCollectionRDD[5] at parallelize at <console>:35\n",
       "flattenWordTuples = MapPartitionsRDD[6] at flatMap at <console>:37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[6] at flatMap at <console>:37"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var smallerSampleSize = sc.parallelize(newsParse.takeSample(false, 30, System.currentTimeMillis().toInt))\n",
    "var flattenWordTuples = smallerSampleSize.\n",
    "  flatMap{ attr =>\n",
    "          \n",
    "      var titleWords = wordPreProcess(attr(1)).map(\n",
    "          word =>\n",
    "          (word, \"title\", attr(4).toString, attr(5).toString.split(\"\\\\s\")(0))\n",
    "      )\n",
    "      \n",
    "      var headlineWords = wordPreProcess(attr(2)).map(\n",
    "          word =>\n",
    "          (word, \"headine\", attr(4).toString, attr(5).toString.split(\"\\\\s\")(0))\n",
    "      )\n",
    "      \n",
    "      titleWords ++ headlineWords\n",
    "  }\n",
    "flattenWordTuples.persist()\n",
    "\n",
    "print(\"\\nFlatten Word Sample: \")\n",
    "flattenWordTuples.take(1).foreach(print)\n",
    "print(\" -- (Word, 'Title'/'Headline', Topic, Date)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flatten Word Sample (Per Topic): \n",
      "((premier,headine,economy),1)\n",
      "((process,headine,obama),1)\n",
      "((jewell,headine,obama),1)\n",
      "((is,title,microsoft),1)\n",
      "((companies,headine,microsoft),1)\n",
      "((langdana's,headine,economy),1)\n",
      "(($43b,title,economy),1)\n",
      "((bullard,headine,economy),1)\n",
      "((other,headine,obama),1)\n",
      "((shove,title,microsoft),1)\n",
      " -- ((Word, 'Title'/'Headline', Topic), Count)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "perTopicTuple = ShuffledRDD[8] at reduceByKey at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[8] at reduceByKey at <console>:41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var perTopicTuple = flattenWordTuples.\n",
    "  map{ case (word, toh, topic, date) =>\n",
    "    ((word, toh, topic), 1)\n",
    "  }.reduceByKey{\n",
    "      (j, k) =>\n",
    "      j+k\n",
    "  }\n",
    "\n",
    "print(\"\\nFlatten Word Sample (Per Topic): \\n\")\n",
    "perTopicTuple.take(10).foreach(println)\n",
    "print(\" -- ((Word, 'Title'/'Headline', Topic), Count)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Output (Per Topic): \n",
      "to,12\n",
      "the,5\n",
      "and,4\n",
      "microsoft,4\n",
      "10,3\n",
      "is,3\n",
      "will,3\n",
      "windows,3\n",
      "a,2\n",
      "edge,2\n",
      " -- \" Word, Count \" \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "perTopicOutput = MapPartitionsRDD[11] at mapValues at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[11] at mapValues at <console>:45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "var perTopicOutput = perTopicTuple.\n",
    "  map{ case ((word, toh, topic), count) =>\n",
    "    ((toh, topic), (word, count))\n",
    "  }.groupByKey.mapValues{ iterator =>\n",
    "      iterator.toVector.sortBy { case(word, count) => \n",
    "          (-count, word)\n",
    "      }.map{\n",
    "          case(word,count) => word + \",\"+ count\n",
    "      }.mkString(\"\\n\")\n",
    "  }\n",
    "\n",
    "perTopicOutput.foreach{ case ((toh, topic), vect_string) =>\n",
    "   new PrintWriter(\"./output/topic/\"+topic+\"_\"+toh+\".csv\") { try {write(vect_string)} finally {close()} }\n",
    "}\n",
    "\n",
    "print(\"\\nSample Output (Per Topic): \\n\")\n",
    "perTopicOutput.first()._2.split(\"\\n\").take(10).foreach(println)\n",
    "print(\"\"\" -- \" Word, Count \" \"\"\"+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flatten Word Sample (Per Day): \n",
      "((embargo,title,2016-05-22),1)\n",
      "((as,headine,2016-03-15),2)\n",
      "((bank,title,2015-12-29),1)\n",
      "((directed,headine,2016-03-15),1)\n",
      "((press,headine,2016-05-22),1)\n",
      "((advisors,headine,2016-03-16),1)\n",
      "((different,headine,2016-06-20),1)\n",
      "((to,title,2016-01-13),1)\n",
      "((29,title,2015-12-29),1)\n",
      "((between,title,2016-05-02),1)\n",
      " -- ((Word, 'Title'/'Headline', Date), Count)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "perDayTuple = ShuffledRDD[13] at reduceByKey at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[13] at reduceByKey at <console>:42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var perDayTuple = flattenWordTuples.\n",
    "  map{ case (word, toh, topic, date) =>\n",
    "    ((word, toh, date), 1)\n",
    "  }.reduceByKey{\n",
    "      (j, k) =>\n",
    "      j+k\n",
    "  }\n",
    "\n",
    "print(\"\\nFlatten Word Sample (Per Day): \\n\")\n",
    "perDayTuple.take(10).foreach(println)\n",
    "print(\" -- ((Word, 'Title'/'Headline', Date), Count)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Output (Per Day): \n",
      "to,12\n",
      "the,5\n",
      "and,4\n",
      "microsoft,4\n",
      "10,3\n",
      "is,3\n",
      "will,3\n",
      "windows,3\n",
      "a,2\n",
      "edge,2\n",
      " -- \" Word, Count \" \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "perDayOutput = MapPartitionsRDD[16] at mapValues at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[16] at mapValues at <console>:50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "var perDayOutput = perDayTuple.\n",
    "  map{ case ((word, toh, date), count) =>\n",
    "    ((toh, date), (word, count))\n",
    "  }.groupByKey.mapValues{ iterator =>\n",
    "      iterator.toVector.sortBy { case(word, count) => \n",
    "          (-count, word)\n",
    "      }.map{\n",
    "          case(word,count) => word + \",\"+ count\n",
    "      }.mkString(\"\\n\")\n",
    "  }\n",
    "\n",
    "perDayOutput.foreach{ case ((toh, date), vect_string) =>\n",
    "   new PrintWriter(\"./output/date/\"+date+\"_\"+toh+\".csv\") { try {write(vect_string)} finally {close()} }\n",
    "}\n",
    "\n",
    "print(\"\\nSample Output (Per Day): \\n\")\n",
    "perTopicOutput.first()._2.split(\"\\n\").take(10).foreach(println)\n",
    "print(\"\"\" -- \" Word, Count \" \"\"\"+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flatten Word Sample (In Total): \n",
      "((service,headine),1)\n",
      "((calls,headine),1)\n",
      "((maher's,title),1)\n",
      "((settlers,title),1)\n",
      "((crises,title),1)\n",
      "((backing,headine),1)\n",
      "((jaitley,headine),1)\n",
      "((solution,headine),1)\n",
      "((out,headine),1)\n",
      "((slower,headine),1)\n",
      " -- ((Word, 'Title'/'Headline'), Count)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "totalTuple = ShuffledRDD[18] at reduceByKey at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[18] at reduceByKey at <console>:45"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var totalTuple = perTopicTuple.\n",
    "  map{ case ((word, toh, topic), count) =>\n",
    "    ((word, toh), 1)\n",
    "  }.reduceByKey{\n",
    "      (j, k) =>\n",
    "      j+k\n",
    "  }\n",
    "\n",
    "print(\"\\nFlatten Word Sample (In Total): \\n\")\n",
    "totalTuple.take(10).foreach(println)\n",
    "print(\" -- ((Word, 'Title'/'Headline'), Count)\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalOutput = MapPartitionsRDD[21] at mapValues at <console>:49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[21] at mapValues at <console>:49"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "var totalOutput = totalTuple.\n",
    "  map{ case ((word, toh), count) =>\n",
    "    (toh, (word, count))\n",
    "  }.groupByKey.mapValues{ iterator =>\n",
    "      iterator.toVector.sortBy { case(word, count) => \n",
    "          (-count, word)\n",
    "      }.map{\n",
    "          case(word,count) => word + \",\"+ count\n",
    "      }.mkString(\"\\n\")\n",
    "  }\n",
    "\n",
    "totalOutput.foreach{ case (toh, vect_string) =>\n",
    "   new PrintWriter(\"./output/total_\"+toh+\".csv\") { try {write(vect_string)} finally {close()} }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Co-occurance Matrices for the Top-100 Frequent Words in Headline and Title in each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Top 15 Words (Per Topic): \n",
      "(obama,Vector(obama, to, the, in, president, a, for, has, on, and, atlantic, been, blame, clinton, drilling))\n",
      "(microsoft,Vector(the, microsoft, to, a, is, of, on, corporate, could, in, nadella, quot, that, windows, 10))\n",
      "(economy,Vector(the, economy, of, and, in, to, with, a, as, for, is, outlook, this, about, growth))\n",
      "(palestine,Vector(liberation, organization, palestine, the, and, appeared, chief, cnn, criticized, fatally, for, how, israeli, of, on))\n",
      " -- (topic, Vector[String]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sizeOfTop = 15\n",
       "perTopicTop100 = Map(obama -> Vector(obama, to, the, in, president, a, for, has, on, and, atlantic, been, blame, clinton, drilling), microsoft -> Vector(the, microsoft, to, a, is, of, on, corporate, could, in, nadella, quot, that, windows, 10), economy -> Vector(the, economy, of, and, in, to, with, a, as, for, is, outlook, this, about, growth), palestine -> Vector(liberation, organization, palestine, the, and, appeared, chief, cnn, criticized, fatally, for, how, israeli, of, on))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(obama -> Vector(obama, to, the, in, president, a, for, has, on, and, atlantic, been, blame, clinton, drilling), microsoft -> Vector(the, microsoft, to, a, is, of, on, corporate, could, in, nadella, quot, that, windows, 10), economy -> Vector(the, economy, of, and, in, to, with, a, as, for, is, outlook, this, about, growth), palestine -> Vector(liberation, organization, palestine, the, and, appeared, chief, cnn, criticized, fatally, for, how, israeli, of, on))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sizeOfTop = 15\n",
    "\n",
    "var perTopicTop100 = perTopicTuple.\n",
    "  map{ case ((word, toh, topic), count) =>\n",
    "    ((word, topic), count)\n",
    "  }.reduceByKey{\n",
    "      case (i, j) => i + j\n",
    "  }.\n",
    "  map{ case ((word, topic), count) =>\n",
    "    (topic, (word, count))\n",
    "  }.groupByKey.mapValues{ iterator =>\n",
    "      iterator.toVector.sortBy { case(word, count) => \n",
    "          (-count, word)\n",
    "      }.take(sizeOfTop).map{case(word, count) => word}\n",
    "  }.collectAsMap()\n",
    "\n",
    "print(\"\\nSample Top \"+sizeOfTop+\" Words (Per Topic): \\n\")\n",
    "perTopicTop100.take(4).foreach(println)\n",
    "print(\" -- (topic, Vector[String]) \"+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurance of Top 15 Words in economy (Headline & Title):\n",
      "(economy,CompactBuffer(List(1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), List(0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), List(1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0), List(1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0), List(1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1), List(1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0), List(0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0), List(1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1), List(1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1), List(1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0), List(1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0), List(1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0), List(1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0), List(1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0)))\n",
      "\n",
      "Co-Occurance Matrix for Top 15 Words in economy (Headline & Title):\n",
      "+-------+---+-------+---+---+---+---+----+---+---+---+---+-------+----+-----+------+\n",
      "|    _c0|the|economy| of|and| in| to|with|  a| as|for| is|outlook|this|about|growth|\n",
      "+-------+---+-------+---+---+---+---+----+---+---+---+---+-------+----+-----+------+\n",
      "|    the| 12|     12|  7|  8|  6|  7|   5|  5|  3|  3|  5|      2|   3|    2|     3|\n",
      "|economy| 12|     14|  7|  8|  6|  7|   5|  6|  5|  4|  5|      2|   3|    2|     3|\n",
      "|     of|  7|      7|  7|  5|  3|  4|   3|  3|  3|  1|  3|      1|   1|    1|     1|\n",
      "|    and|  8|      8|  5|  8|  3|  5|   5|  3|  2|  3|  2|      2|   1|    1|     3|\n",
      "|     in|  6|      6|  3|  3|  6|  4|   2|  4|  1|  1|  4|      0|   3|    1|     2|\n",
      "|     to|  7|      7|  4|  5|  4|  7|   3|  3|  2|  1|  3|      0|   2|    0|     1|\n",
      "|   with|  5|      5|  3|  5|  2|  3|   5|  3|  1|  1|  2|      1|   1|    0|     2|\n",
      "|      a|  5|      6|  3|  3|  4|  3|   3|  6|  2|  2|  3|      1|   2|    0|     2|\n",
      "|     as|  3|      5|  3|  2|  1|  2|   1|  2|  5|  1|  0|      0|   1|    0|     0|\n",
      "|    for|  3|      4|  1|  3|  1|  1|   1|  2|  1|  4|  0|      2|   0|    1|     2|\n",
      "|     is|  5|      5|  3|  2|  4|  3|   2|  3|  0|  0|  5|      0|   2|    1|     1|\n",
      "|outlook|  2|      2|  1|  2|  0|  0|   1|  1|  0|  2|  0|      2|   0|    1|     1|\n",
      "|   this|  3|      3|  1|  1|  3|  2|   1|  2|  1|  0|  2|      0|   3|    1|     0|\n",
      "|  about|  2|      2|  1|  1|  1|  0|   0|  0|  0|  1|  1|      1|   1|    2|     0|\n",
      "| growth|  3|      3|  1|  3|  2|  1|   2|  2|  0|  2|  1|      1|   0|    0|     3|\n",
      "+-------+---+-------+---+---+---+---+----+---+---+---+---+-------+----+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "perTopicOccurance = ShuffledRDD[48] at groupByKey at <console>:60\n",
       "exportMatrix = MapPartitionsRDD[49] at map at <console>:62\n",
       "df_2 = [_c0: string, the: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, the: string ... 14 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "var perTopicOccurance = smallerSampleSize.\n",
    "  map{ attr =>\n",
    "      var titleAndHeadline = wordPreProcess(attr(1).toString + attr(2).toString)\n",
    "      var list =  ListBuffer[Int]()\n",
    "      perTopicTop100(attr(4)).foreach{ word =>\n",
    "          list += (if (titleAndHeadline.contains(word)) 1 else 0)\n",
    "      }\n",
    "      (attr(4),list.toSeq)\n",
    "  }.groupByKey\n",
    "\n",
    "var exportMatrix = perTopicOccurance.map{\n",
    "      case (topic, iterator) =>\n",
    "      val matrix = Array.ofDim[Int](sizeOfTop,sizeOfTop)\n",
    "      val occur_matrix = Array.ofDim[Int](sizeOfTop,sizeOfTop)\n",
    "      val result_matrix = Array.ofDim[Double](sizeOfTop,sizeOfTop)\n",
    "      iterator.map{\n",
    "          list =>\n",
    "          val indexWithValue = list.zipWithIndex.filter(_._1 != 0).map(_._2)\n",
    "          for( x <- 0 until indexWithValue.length ; y <- 0 until indexWithValue.length ){\n",
    "              matrix(indexWithValue(x))(indexWithValue(y)) += 1\n",
    "          }\n",
    "          \n",
    "      }\n",
    "      val resultString = \",\"+perTopicTop100(topic).mkString(\",\") +\"\\n\" + \n",
    "                         matrix.zipWithIndex.map{ case(x,i) => perTopicTop100(topic)(i)+\n",
    "                         \",\"+x.mkString(\",\")}.mkString(\"\\n\")\n",
    "      (topic,resultString)\n",
    "  }\n",
    "\n",
    "exportMatrix.foreach{\n",
    "    case (topic, resultString) =>\n",
    "    new PrintWriter(\"./output/\"+topic+\"_co-occurance.csv\") { try {write(resultString)} finally {close()} }\n",
    "}\n",
    "\n",
    "println(\"Occurance of Top \"+sizeOfTop+\" Words in \"+perTopicOccurance.first()._1+\" (Headline & Title):\")\n",
    "println(perTopicOccurance.first()+\"\\n\")\n",
    "\n",
    "var df_2 = spark.read.\n",
    "  format(\"csv\").\n",
    "  option(\"header\", \"true\").\n",
    "  csv(\"./output/\"+perTopicOccurance.first()._1+\"_co-occurance.csv\")\n",
    "\n",
    "println(\"Co-Occurance Matrix for Top \"+sizeOfTop+\" Words in \"+perTopicOccurance.first()._1+\" (Headline & Title):\")\n",
    "df_2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
