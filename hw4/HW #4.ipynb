{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "[Reuters-21578 Text Categorization Collection Data Set](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection) - about 2 million instances, 28MB in size\n",
    "\n",
    "\n",
    "News: 21 SGML files\n",
    "We only deal with news contents inside <body> </body> tags\n",
    "The other files will not be needed in this homework\n",
    "\n",
    "### Format\n",
    "One text file consisting of lines of records.\n",
    "\n",
    "Each record contains 9 attributes separated by semicolons: \n",
    "\n",
    "\n",
    "Reuters-21578, Distribution 1.0 includes five files (all-exchanges-strings.lc.txt, all-orgs-strings.lc.txt, all-people-strings.lc.txt, all-places-strings.lc.txt, and all-topics-strings.lc.txt) which list the names of *all* legal categories in each set. A sixth file, cat-descriptions_120396.txt gives some additional information on the category sets.\n",
    "\n",
    "\n",
    "### Task\n",
    "4 subtasks:\n",
    "+ (30pt) Given the Reuters-21578 dataset, please calculate the term frequencies, and output the representation of the document contents as a term-document count matrix.\n",
    "+ (30pt) Implement matrix multiplication by MapReduce. Your program should be able to output the result in appropriate dimensions.\n",
    "+ (40pt) Given the term-document matrix in (1), compute the SVD decomposition of the matrix using MapReduce. Output the resulting eigenvalues and eigenvectors.\n",
    "+ (Bonus) Given the term-document matrix in (1), compute the CUR decomposition of the matrix using MapReduce. Output the resulting eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Format\n",
    "\n",
    "1. Term-Document Matrix:\n",
    "A MxN matrix: with rows as term frequencies and columns as documents (N=21,578) \n",
    "2. Result of Matrix Multiplication:\n",
    "A MxR matrix: (MxN) * (NxR)\n",
    "3. Eigen Pairs:\n",
    "Eigenvalues sorted in descending order, and their corresponding eigenvectors\n",
    "4. Eigen Pairs: \n",
    "Eigenvalues sorted in descending order, and their corresponding eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Entity:       org.apache.spark.sql.SparkSession@5cf7d99a\n",
      "Spark version:      2.3.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "printSample: (writer: Any, data: Any, title: String, format: String)Unit\n",
       "printSpark: (writer: Any, spark: org.apache.spark.sql.SparkSession)Unit\n",
       "outputWriter: (fileString: String)java.io.PrintWriter\n",
       "getFile: (fileString: String)Array[String]\n",
       "writer: Null = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.hadoop.fs._\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import java.io.{File,PrintWriter}\n",
    "\n",
    "  def printSample(writer: Any, data: Any, title: String = \"\", format: String = \"\"){\n",
    "    println(\"\\n\"+title+\" Data Sample: \" + format)\n",
    "    println(data+\"\\n\")\n",
    "  }\n",
    "\n",
    "  def printSpark(writer: Any, spark: SparkSession): Unit = {\n",
    "    println(\"Spark Entity:       \" + spark)\n",
    "    println(\"Spark version:      \" + spark.version)\n",
    "    println(\"Spark master:       \" + spark.sparkContext.master)\n",
    "    println(\"Running 'locally'?: \" + spark.sparkContext.isLocal)\n",
    "    println(\"\")\n",
    "  }\n",
    "\n",
    "  def outputWriter(fileString: String): PrintWriter ={\n",
    "    val outputPath = new Path(fileString)\n",
    "    val outputStream = outputPath.getFileSystem(new Configuration()).create(outputPath);\n",
    "    new PrintWriter(outputStream)\n",
    "  }\n",
    "\n",
    "  def getFile(fileString: String): Array[String] ={\n",
    "    val inputPath = new Path(fileString)\n",
    "    val inputBuffer = scala.collection.mutable.ArrayBuffer.empty[String]\n",
    "    val iterator = inputPath.getFileSystem(new Configuration()).listFiles(inputPath, false)\n",
    "    while(iterator.hasNext()){\n",
    "        val fileStatus = iterator.next()\n",
    "        if(fileStatus.isFile()){\n",
    "          inputBuffer += fileStatus.getPath().toString()\n",
    "        }\n",
    "    }\n",
    "    inputBuffer.toArray\n",
    "  }\n",
    "\n",
    "\n",
    "val writer = null\n",
    "printSpark(writer, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@5cf7d99a\n",
       "data = Array(file:/home/micky/school/big_data_mining/hw4/data/reut2-009.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-008.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-012.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-018.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-020.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-010.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-004.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-002.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-006.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-014.sgm, file:/home/micky/school/big_data_mining/hw...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[file:/home/micky/school/big_data_mining/hw4/data/reut2-009.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-008.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-012.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-018.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-020.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-010.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-004.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-002.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-006.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-014.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-000.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-013.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-007.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-005.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-016.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-015.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-003.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-021.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-001.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-019.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-011.sgm, file:/home/micky/school/big_data_mining/hw4/data/reut2-017.sgm]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark2 = spark\n",
    "\n",
    "var data = getFile(\"./data\")\n",
    "var sample_data = Array(\"./data/reut2-017.sgm\", \"./data/reut2-018.sgm\", \"./data/reut2-019.sgm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count: 2749\n",
      "\n",
      "Parsed Sample Data Sample: \n",
      "(17001,The Brazilian Coffee Institute, IBC,\n",
      "plans to sell in a series of auctions over the next few weeks\n",
      "robusta coffee purchased in London last year, but details of\n",
      "where and when auctions will take place are still to be\n",
      "finalised, IBC president Jorio Dauster told reporters.\n",
      "    The sales of 630,000 bags of robusta and an unspecified\n",
      "amount of Brazilian arabica coffee will take place over a\n",
      "minimum of six months but it is not decided where sales will\n",
      "take place or whether they will be held weekly or monthly.\n",
      "    The amount offered at each sale has also not been set, but\n",
      "could be in the order of 100,000 bags, Dauster said.)\n",
      "\n",
      "(17002,AMCA International Ltd said it\n",
      "appointed president and chief executive officer WIlliam Holland\n",
      "to succeed Kenneth Barclay as chairman.\n",
      "    Barclay, who is 60 years old, decided not to stand for\n",
      "reappointment as chairman this year but will continue as a\n",
      "director, AMCA said.)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "regex = (?s)<REUTERS.*?NEWID=\"(\\d*)\".*?>.*?<TEXT.*?>(.*?)<\\/TEXT>.*?<\\/REUTERS>\n",
       "body_regex = (?s).*?<BODY.*?>(.*?)<\\/BODY>.*?\n",
       "newsParse = UnionRDD[12] at union at <console>:59\n",
       "newsCount = 2749\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "//TODO: Sample -> Actual\n",
    "var regex = \"\"\"(?s)<REUTERS.*?NEWID=\"(\\d*)\".*?>.*?<TEXT.*?>(.*?)<\\/TEXT>.*?<\\/REUTERS>\"\"\".r\n",
    "var body_regex = \"\"\"(?s).*?<BODY.*?>(.*?)<\\/BODY>.*?\"\"\".r\n",
    "var newsParse = spark.sparkContext.emptyRDD[(Int, String)]\n",
    "\n",
    "sample_data.map{\n",
    "    path  =>\n",
    "    val singleNewsParse = spark.sparkContext.wholeTextFiles(path).flatMap{\n",
    "        case (_, string) =>\n",
    "        val list =  ListBuffer[Tuple2[Int, String]]()\n",
    "        for(m <- regex.findAllIn(string).matchData){\n",
    "            for( n <- body_regex.findAllIn(m.group(2)).matchData){\n",
    "                list += (Tuple2(m.group(1).toInt, n.group(1).replace(\"\\n Reuter\\n&#3;\",\"\")))\n",
    "            }\n",
    "        }\n",
    "        list.toSeq\n",
    "    }\n",
    "    newsParse = newsParse.union(singleNewsParse)\n",
    "}\n",
    "var newsCount = newsParse.count()\n",
    "print(\"\\nCount: \"+ newsCount+\"\\n\")\n",
    "printSample(writer, newsParse.take(2).mkString(\"\\n\\n\"), \"Parsed Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Term-Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ziped With Index Sample Data Sample: \n",
      "18063 -> 992\n",
      "17408 -> 402\n",
      "18404 -> 1314\n",
      "17749 -> 711\n",
      "18090 -> 1013\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newsCount = Broadcast(6)\n",
       "matrixIndex = Broadcast(11)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "wordPreProcess: (input: Any)Array[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Broadcast(11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordPreProcess(input: Any): Array[String] = {\n",
    "        var matchRegex = \"\"\"([$]?(?:[\\w]+(?:[\\w',]*[\\w]+)+|[\\w]))\"\"\".r\n",
    "        var list =  ListBuffer[String]()\n",
    "        for(m <- matchRegex.findAllIn(input.toString.toLowerCase).matchData;\n",
    "          e <- m.subgroups)\n",
    "          list+=e\n",
    "        list.toArray\n",
    "    }\n",
    "\n",
    "var newsCount = spark.sparkContext.broadcast(newsParse.count())\n",
    "var matrixIndex = spark.sparkContext.broadcast(newsParse.map{\n",
    "    case(id, string) =>\n",
    "    id.toInt\n",
    "}.sortBy{\n",
    "    case (id) => id\n",
    "}.zipWithIndex.collectAsMap())\n",
    "\n",
    "printSample(writer, matrixIndex.value.take(5).mkString(\"\\n\"), \"Ziped With Index Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Shingle Sample Data Sample: \n",
      "(francs,CompactBuffer((1271,1), (2342,1), (859,7), (1363,1), (2049,2), (455,1), (2376,2), (139,1), (1741,1), (526,2), (856,8), (57,2), (1890,1), (2319,1), (376,1), (143,3), (2089,2), (1361,1), (1906,4), (695,2), (1467,2), (945,1), (1419,1), (270,1), (1429,8), (1942,4), (944,4), (1432,1), (84,3), (2324,2), (1581,1), (1448,2), (635,1), (2352,5)))\n",
      "(constraints,CompactBuffer((913,1), (240,1), (837,1), (1907,1), (883,1), (1812,1), (868,1)))\n",
      "(obliging,CompactBuffer((241,1)))\n",
      "(brewery,CompactBuffer((1264,1), (123,1)))\n",
      "(topped,CompactBuffer((1847,1), (856,1), (721,2), (998,1), (2237,1)))\n",
      "(wla,CompactBuffer((2112,1)))\n",
      "(datacard,CompactBuffer((2455,9)))\n",
      "(dole,CompactBuffer((2698,2), (344,2), (79,1), (690,1), (993,3)))\n",
      "(gowns,CompactBuffer((2575,1)))\n",
      "(dayton,CompactBuffer((2266,2), (2079,4), (2220,7), (607,1)))\n",
      "(albertson,CompactBuffer((1714,1)))\n",
      "(fred,CompactBuffer((2203,1), (353,1), (2138,1), (2037,1)))\n",
      "(preventing,CompactBuffer((873,1), (2327,1), (2455,1), (422,1), (495,1), (388,1)))\n",
      "(barring,CompactBuffer((1877,1), (2105,1), (351,1), (1938,1), (1068,1), (2566,1), (1254,1)))\n",
      "(17a,CompactBuffer((699,1)))\n",
      "(489,351,CompactBuffer((1102,1)))\n",
      "(262,242,CompactBuffer((10,1)))\n",
      "(accomplished,CompactBuffer((487,1), (122,1)))\n",
      "(exploded,CompactBuffer((794,1), (456,2)))\n",
      "(1,900,CompactBuffer((2312,1), (418,1)))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flattenTerms = ShuffledRDD[23] at groupByKey at <console>:55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[23] at groupByKey at <console>:55"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var flattenTerms = newsParse.flatMap{\n",
    "    case (id, string)  =>\n",
    "    wordPreProcess(string).map{\n",
    "        word =>\n",
    "        ((word, matrixIndex.value(id).toInt), 1)\n",
    "    }  \n",
    "}.reduceByKey{\n",
    "    (x, y) => x + y\n",
    "}.map{\n",
    "    case((word, id),count) =>\n",
    "    (word,(id, count))\n",
    "}.groupByKey()\n",
    "printSample(writer, flattenTerms.take(20).mkString(\"\\n\"), \"Parsed Shingle Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termDocumentMatrix = MapPartitionsRDD[24] at map at <console>:44\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[24] at map at <console>:44"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var termDocumentMatrix = flattenTerms.map{\n",
    "    case (word, iterable) =>\n",
    "    var row = Array.fill(newsCount.value.toInt){0}\n",
    "    iterable.toVector.map{\n",
    "        case(id, count) =>\n",
    "        row(id) = count\n",
    "    }\n",
    "    row\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termDocumentString = MapPartitionsRDD[25] at map at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[25] at map at <console>:46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termDocumentString = termDocumentMatrix.map{\n",
    "      array =>\n",
    "      array.mkString(\",\")\n",
    "}\n",
    "\n",
    "termDocumentString.saveAsTextFile(\"./output/termDocumentString\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand = scala.util.Random$@15f1c2d1\n",
       "r_value = 3\n",
       "otherMatrix = ParallelCollectionRDD[25] at parallelize at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[25] at parallelize at <console>:42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rand = scala.util.Random\n",
    "val r_value = 3\n",
    "var otherMatrix = spark.sparkContext.parallelize(Array.fill(newsCount.value.toInt,r_value){rand.nextInt(2000) - 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrixMultiplication: (matrix1: org.apache.spark.rdd.RDD[Array[Int]], matrix2: org.apache.spark.rdd.RDD[Array[Int]])org.apache.spark.rdd.RDD[Array[Int]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "def matrixMultiplication(matrix1: RDD[Array[Int]], matrix2: RDD[Array[Int]]): RDD[Array[Int]] = {\n",
    "    var otherMatrix = spark.sparkContext.broadcast(matrix2.zipWithIndex.map{x => (x._2, x._1)}.collectAsMap())\n",
    "    matrix1.map{\n",
    "        array =>\n",
    "        var newRow = Array.fill(otherMatrix.value(0).length){0}\n",
    "        newRow.zipWithIndex.map{\n",
    "            case(value, columnid) =>\n",
    "            var newValue = value\n",
    "            array.zipWithIndex.foreach{\n",
    "                case(arrayValue, rowid) =>\n",
    "                newValue += arrayValue * otherMatrix.value(rowid)(columnid)\n",
    "            }\n",
    "            newValue\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16610, -9520, 1261\n",
      "-1182, 695, -1421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "multipliedMatrix = MapPartitionsRDD[28] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[28] at map at <console>:47"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var multipliedMatrix = matrixMultiplication(termDocumentMatrix, otherMatrix)\n",
    "\n",
    "multipliedMatrix.take(2).foreach{\n",
    "    array =>\n",
    "    println(array.mkString(\", \"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mulString = MapPartitionsRDD[31] at map at <console>:55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[31] at map at <console>:55"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var mulString = multipliedMatrix.map{\n",
    "      array =>\n",
    "      array.mkString(\",\")\n",
    "}\n",
    "\n",
    "mulString.saveAsTextFile(\"./output/multipliedMatrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrixDoubleMultiplication: (matrix1: org.apache.spark.rdd.RDD[Array[Double]], matrix2: org.apache.spark.rdd.RDD[Array[Double]])org.apache.spark.rdd.RDD[Array[Double]]\n",
       "matrixSubtract: (matrix1: org.apache.spark.rdd.RDD[Array[Double]], matrix2: org.apache.spark.rdd.RDD[Array[Double]])org.apache.spark.rdd.RDD[Array[Double]]\n",
       "matrixTranspose: (matrix: org.apache.spark.rdd.RDD[Array[Double]])org.apache.spark.rdd.RDD[Array[Double]]\n",
       "matrixAbs: (matrix: org.apache.spark.rdd.RDD[Array[Double]])Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import math.{sqrt,pow}\n",
    "\n",
    "def matrixDoubleMultiplication(matrix1: RDD[Array[Double]], matrix2: RDD[Array[Double]]): RDD[Array[Double]] = {\n",
    "    var otherMatrix = spark.sparkContext.broadcast(matrix2.zipWithIndex.map{x => (x._2, x._1)}.collectAsMap())\n",
    "    matrix1.map{\n",
    "        array =>\n",
    "        var newRow = Array.fill(otherMatrix.value(0).length){0.0}\n",
    "        newRow.zipWithIndex.map{\n",
    "            case(value, columnid) =>\n",
    "            var newValue = value\n",
    "            array.zipWithIndex.foreach{\n",
    "                case(arrayValue, rowid) =>\n",
    "                newValue += arrayValue * otherMatrix.value(rowid)(columnid)\n",
    "            }\n",
    "            newValue \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def matrixSubtract(matrix1: RDD[Array[Double]], matrix2: RDD[Array[Double]]): RDD[Array[Double]] = {\n",
    "    var otherMatrix = spark.sparkContext.broadcast(matrix2.zipWithIndex.map{x => (x._2, x._1)}.collectAsMap())\n",
    "    matrix1.zipWithIndex.map{\n",
    "        case (array, rowid) =>\n",
    "        array.zipWithIndex.map{\n",
    "            case(value, columnid) =>\n",
    "            value - otherMatrix.value(rowid)(columnid)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def matrixTranspose(matrix: RDD[Array[Double]]): RDD[Array[Double]] = {\n",
    "    var transpose = matrix.collect.toSeq.transpose.map{seq => seq.toArray}\n",
    "    spark.sparkContext.parallelize(transpose)\n",
    "}\n",
    "\n",
    "\n",
    "def matrixAbs(matrix: RDD[Array[Double]]): Double = {\n",
    "    var sum = matrix.map{\n",
    "        array => \n",
    "        array.map{\n",
    "            value =>\n",
    "            pow(value, 2)\n",
    "        }\n",
    "    }.reduce((a,b) => Array.fill(1){a(0) + b(0)})\n",
    "    sqrt(sum(0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32: 3.0275189385964336E10\n",
      "5: -2.543515231731319E52\n",
      "11: 1.0645638478974878E262\n",
      "3: NaN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count = 4\n",
       "doubleTermMatrix = MapPartitionsRDD[1538] at map at <console>:61\n",
       "transposeMatrix = ParallelCollectionRDD[1539] at parallelize at <console>:69\n",
       "symmetricMatrix = MapPartitionsRDD[1869] at map at <console>:58\n",
       "eigenValue = NaN\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var count = 0\n",
    "var doubleTermMatrix = termDocumentMatrix.map{array => array.map{x => x.toDouble}}\n",
    "var transposeMatrix = matrixTranspose(doubleTermMatrix)\n",
    "var symmetricMatrix = matrixDoubleMultiplication(doubleTermMatrix, transposeMatrix)\n",
    "\n",
    "var eigenValue = 0.0\n",
    "\n",
    "while(count < 5 && !eigenValue.isNaN){  \n",
    "    var xMatrix = spark.sparkContext.parallelize(Array.fill(symmetricMatrix.count().toInt,1){1.0})\n",
    "    var prevDiv = 0.0\n",
    "    var div = 10.0\n",
    "    var countNum = 0\n",
    "\n",
    "    while(abs(div-prevDiv) >= 0.00000000000001){\n",
    "        countNum += 1\n",
    "        prevDiv = div\n",
    "        var mul = matrixDoubleMultiplication(symmetricMatrix, xMatrix) \n",
    "        div = matrixAbs(xMatrix)\n",
    "\n",
    "        xMatrix = mul.map{\n",
    "            array=> \n",
    "            array.map{ value => value/div }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    var transposeX = matrixTranspose(xMatrix)\n",
    "    var partial = matrixDoubleMultiplication(transposeX, symmetricMatrix)\n",
    "    var eigen = matrixDoubleMultiplication(partial, xMatrix)\n",
    "    eigenValue = eigen.collect()(0)(0)\n",
    "\n",
    "    println(countNum +\": \"+eigenValue)\n",
    "    xMatrix.map{array => array.mkString(\", \")}.saveAsTextFile(\"./output/SVD/\"+eigenValue)\n",
    "\n",
    "    var partial2 = matrixDoubleMultiplication(xMatrix, transposeX)\n",
    "    var partial3 = partial2.map{\n",
    "        array=>\n",
    "        array.map{ value => value * eigenValue }\n",
    "    }\n",
    "    \n",
    "    symmetricMatrix = matrixSubtract(symmetricMatrix, partial3)\n",
    "    \n",
    "    count += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "103820004 Michael Fu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
